BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS)
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Seminar: Daniel Roy (University of Toronto)
DTSTART;VALUE=DATE-TIME:20201118T170000
DTEND;VALUE=DATE-TIME:20201118T180000
DESCRIPTION:Event description:\n“In Defense of Uniform Convergence: Gene
 ralization via derandomization with an application to interpolating predic
 tors”\n\nSpeakers:\n\nDaniel Roy\n\nAssociate Professor\, University of 
 Toronto\n\nJeffrey Negrea\n\nPhD Candidate\n\nTo participate:\n\nJoin from
  PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.us/j/95827054840\n\
 nOr Telephone：203-432-9666 (2-ZOOM if on-campus) or 646 568 7788\nMeetin
 g ID: 958 2705 4840\nInternational numbers available:\nhttps://yale.zoom.u
 s/u/aciY0peggr\n\nTalk Summary:\nWe propose to study the generalization er
 ror of a learned predictor ^h in terms of that of a surrogate (potentially
  randomized) predictor that is coupled to ^h and designed to trade empiric
 al risk for control of generalization error. In the case where ^h interpol
 ates the data\, it is interesting to consider theoretical surrogate classi
 fiers that are partially derandomized or rerandomized\, e.g.\, fit to the 
 training data but with modified label noise. We also show that replacing ^
 h by its conditional distribution with respect to an arbitrary σ-field is
  a convenient way to derandomize. We study two examples\, inspired by the 
 work of Nagarajan and Kolter (2019) and Bartlett et al. (2019)\, where the
  learned classifier ^h interpolates the training data with high probabilit
 y\, has small risk\, and\, yet\, does not belong to a nonrandom class with
  a tight uniform bound on two-sided generalization error. At the same time
 \, we bound the risk of ^h in terms of surrogates constructed by condition
 ing and denoising\, respectively\, and shown to belong to nonrandom classe
 s with uniformly small generalization error.\n\nJoint work by Jeffrey Negr
 ea\, Gintare Karolina Dziugaite\, Daniel M. Roy\n\n.\n\n.\n\n\nhttps://yin
 s.yale.edu/event/yins-seminar-daniel-roy-university-toronto
LOCATION:Zoom
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-daniel-roy-university-toronto
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Distinguished Lecturer Seminar: Peter Bartlett (UC Berkeley)
DTSTART;VALUE=DATE-TIME:20201202T170000
DTEND;VALUE=DATE-TIME:20201202T180000
DESCRIPTION:Event description:\n“Benign Overfitting”\n\nSpeaker: Peter
  Bartlett\n\nProfessor of Computer Science and Statistics\, University of 
 California at Berkeley\nAssociate Director of the Simons Institute for the
  Theory of Computing\nDirector of the Foundations of Data Science Institut
 e\nDirector of the Collaboration on the Theoretical Foundations of Deep Le
 arning\n\nTo participate:\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\
 nhttps://yale.zoom.us/j/97135219127\nOr Telephone：203-432-9666 (2-ZOOM i
 f on-campus) or 646 568 7788\nMeeting ID: 971 3521 9127\nInternational num
 bers available:\nhttps://yale.zoom.us/u/abxwXKgpCp\n\nTalk Summary:\nClass
 ical theory that guides the design of nonparametric prediction methods lik
 e deep neural networks involves a tradeoff between the fit to the training
  data and the complexity of the prediction rule. Deep learning seems to op
 erate outside the regime where these results are informative\, since deep 
 networks can perform well even with a perfect fit to noisy training data. 
 We investigate this phenomenon of ‘benign overfitting’ in the simplest
  setting\, that of linear prediction. We give a characterization of linear
  regression problems for which the minimum norm interpolating prediction r
 ule has near-optimal prediction accuracy. The characterization is in terms
  of two notions of effective rank of the data covariance. It shows that ov
 erparameterization is essential: the number of directions in parameter spa
 ce that are unimportant for prediction must significantly exceed the sampl
 e size.  It also shows an important role for finite-dimensional data: ben
 ign overfitting occurs for a much narrower range of properties of the data
  distribution when the data lies in an infinite dimensional space versus w
 hen it lies in a finite dimensional space whose dimension grows faster tha
 n the sample size. We discuss implications for deep networks\, for robustn
 ess to adversarial examples\, and for the rich variety of possible behavio
 rs of excess risk as a function of dimension\, and we describe extensions 
 to ridge regression and barriers to analyzing benign overfitting based on 
 model-dependent generalization bounds.  Joint work with Phil Long\, Gábo
 r Lugosi\, and Alex Tsigler.\n\nSpeaker bio:\nPeter Bartlett is professor 
 of Computer Science and Statistics at the University of California at Berk
 eley\, Associate Director of the Simons Institute for the Theory of Comput
 ing\, Director of the Foundations of Data Science Institute\, and Director
  of the Collaboration on the Theoretical Foundations of Deep Learning. His
  research interests include machine learning and statistical learning theo
 ry\, and he is the co-author of the book Neural Network Learning: Theoreti
 cal Foundations. He has been Institute of Mathematical Statistics Medallio
 n Lecturer\, winner of the Malcolm McIntosh Prize for Physical Scientist o
 f the Year\, and Australian Laureate Fellow\, and he is a Fellow of the IM
 S\, Fellow of the ACM\, and Fellow of the Australian Academy of Science.\n
 \n.\n\n\nhttps://yins.yale.edu/event/yins-distinguished-lecturer-seminar-p
 eter-bartlett-uc-berkeley
LOCATION:Join from PC\, Mac\, Linux\, iOS or Android: https://yale.zoom.us
 /j/97135219127
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-distinguished-lecturer-seminar-peter-
 bartlett-uc-berkeley
END:VEVENT
END:VCALENDAR
