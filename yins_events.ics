BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS) Events Calendar
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Distinguished Lecturer: Mikhail Belkin (OSU)
DTSTART;VALUE=DATE-TIME:20200122T170000
DTEND;VALUE=DATE-TIME:20200122T180000
DESCRIPTION:Event description:\n“From classical statistics to modern mac
 hine learning”\n\nSpeaker: Mikhail Belkin\nProfessor in the Department o
 f Computer Science and Engineering and the Department of Statistics at the
  Ohio State University\n\n\n\nAbstract:\n“A model with zero training err
 or is  overfit to the training data and  will typically generalize poorl
 y” goes statistical textbook wisdom. Yet\, in modern practice over-param
 etrized deep networks with near perfect  fit on training data still show 
 excellent test performance. As I will discuss in the talk\, this apparent 
 contradiction is key to understanding the practice of modern machine learn
 ing. While classical methods rely on a trade-off balancing the complexity 
 of  predictors with  training error\,  modern models are best described
  by interpolation\,  where  a predictor is chosen  among functions that
   fit the training data exactly\, according  to a certain (implicit or e
 xplicit) inductive bias. Furthermore\,  classical and modern models can b
 e unified within a single  ”double descent” risk curve\,  which exte
 nds the classical U-shaped  bias-variance curve  beyond the point of int
 erpolation. This understanding of model performance  delineates the limit
 s of the usual ”what you see is what you get” generalization bounds in
  machine learning and  points to  new analyses required to understand  
 computational\, statistical\, and mathematical properties of modern models
 .\n\n\n\nI will proceed to discuss some important implications of interpol
 ation for  optimization\,  both in terms of “easy” optimization due 
 the scarcity of non-global minima\,  and to fast convergence of small min
 i-batch SGD with fixed step size.\n\n\n\nSpeaker bio:\nMikhail Belkin is a
  Professor in the Department of Computer Science and Engineering and the D
 epartment of Statistics at the Ohio State University.  He received his Ph
 D from  the University of Chicago in Mathematics in 2003.  His research 
 focuses on understanding the fundamental structure in data\, the principle
 s of recovering these structures and their computational\, mathematical an
 d statistical properties.\n\nThis understanding\, in turn\, leads to algor
 ithms for dealing with real-world data. His work includes algorithms such 
 as Laplacian Eigenmaps and Manifold Regularization which use ideas of clas
 sical differential geometry for analyzing non-linear high-dimensional data
  and have been widely used in applications.  Prof. Belkin is a recipient 
 of an NSF Career Award and a number of best paper and other awards. He has
  served on the editorial boards of the Journal of Machine Learning Researc
 h and IEEE PAMI.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n
LOCATION:Yale Institute for Network Science
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-distinguished-lecturer-mikhail-belkin
 -osu
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Distinguished Lecturer: Christos Papadimitriou (Columbia)
DTSTART;VALUE=DATE-TIME:20200401T160000
DTEND;VALUE=DATE-TIME:20200401T170000
DESCRIPTION:Event description:\n“Language\, Brain\, and Computation”\n
 \nSpeaker: Christos H. Papadimitriou\nProfessor of Computer Science\, Col
 umbia University\n\nAbstract:\nComputation in the brain has been modeled p
 roductively at many scales\, ranging from molecules to dendrites\, neurons
  and synapses\, all the way to the whole brain models useful in cognitive 
 science.  I will discuss recent work on anl intermediate computational la
 yer\, involving assemblies of neurons — that is to say\, populations of 
 neurons firing together in a repetitive pattern whenever we think of a par
 ticular memory\, concept or idea.  Assemblies were conjectured seven deca
 des ago by Hebb\, and have been over the past two decades noticed in both 
 the animal and the human brain.  Further\, experiments\, simulations\, an
 d theoretical analysis suggest that assemblies can be copied from one brai
 n area to another\, can be associated with other assemblies\, and more.  
 We propose a broader “calculus” of assemblies\, including the operatio
 n “merge”\, comprising a powerful computational model.  One interesti
 ng hypothesis is that assembly operations may underlie some of the most ad
 vanced functions of the human brain\, especially language.  Joint work wi
 th Santosh Vempala\, Wolfgang Maass\, and Michael Collins.\n\n\n\nSpeaker 
 Bio:\nChristos H. Papadimitriou is the Donovan Family professor of compute
 r science at Columbia University.  Before joining Columbia in 2017\, he t
 aught at UC Berkeley for 22 years\, and before that at Harvard\, MIT\, NTU
  Athens\, Stanford\, and UCSD.  He has written five textbooks and many ar
 ticles on algorithms and complexity\, and their applications to optimizati
 on\, databases\, control\, AI\, robotics\, economics and game theory\, the
  Internet\, evolution\, and more recently the study of the brain.  He hol
 ds a PhD from Princeton as well as eight honorary doctorates\, and he has 
 won the Knuth prize\, the Goedel prize\, the von Neumann medal\, and most 
 recently Technion’s Harvey prize.  He is a member of the National Acade
 my of Sciences of the US\, the American Academy of Arts and Sciences\, and
  the National Academy of Engineering\, while in 2013 the president of Gree
 ce named him commander of the order of the phoenix.  He has also written 
 three novels: “Turing”\, “Logicomix” and his latest “Independenc
 e”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n
LOCATION:Yale Institute for Network Science
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-distinguished-lecturer-christos-papad
 imitriou-columbia
END:VEVENT
END:VCALENDAR
