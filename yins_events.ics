BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS)
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Seminar: Boaz Barak (Harvard University)
DTSTART;VALUE=DATE-TIME:20201111T170000
DTEND;VALUE=DATE-TIME:20201111T180000
DESCRIPTION:Event description:\nYINS Distinguished Lecturer Series:\n\n“
 Understanding generalization requires rethinking deep learning?”\n\nSpea
 ker: Boaz Barak\n\nGordon McKay Professor of Computer Science\nHarvard Joh
 n A. Paulson School of Engineering and Applied Sciences\nHarvard Universit
 y\n\nTo participate:\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttp
 s://yale.zoom.us/j/91899440944\nOr Telephone：203-432-9666 (2-ZOOM if on-
 campus) or 646 568 7788\nMeeting ID: 918 9944 0944\nInternational numbers 
 available:\nhttps://yale.zoom.us/u/abfSsSysEh\n\nTalk summary:\nIn classic
 al statistical learning theory\, we can place bounds on the generalization
  gap - the difference between the empirical performance of a learned class
 ifier on its training set and the population performance on unseen test ex
 amples. Such bounds are hard to prove for deep learning. There is also emp
 irical evidence that they are simply not true and deep-learning algorithms
  actually do have non-vanishing generalization gaps.\n\nIn this talk we wi
 ll see that there is a variant of supervised deep learning that does have 
 small generalization gaps\, both in practice and in theory. This variant i
 s “Self-Supervised + Simple fit” (SSS) algorithms that are obtained by
  first using self-supervision to learn a complex representation of the (la
 bel free) training data\, and then fitting a simple (e.g.\, linear) classi
 fier to the labels. Such classifiers have become increasingly popular in r
 ecent years\, as they offer several practical advantages and have been sho
 wn to approach state-of-art results.\n\nWe show that (under assumptions de
 scribed below) the generalization gap of such classifiers tends to zero as
  long as the complexity of the simple classifier is asymptotically smaller
  than the number of training samples. Our bound is independent of the comp
 lexity of the representation\,  which can use an arbitrarily large number
  of parameters. Our bound holds assuming that the learning algorithm satis
 fies certain noise-robustness (adding a small amount of label noise causes
  small degradation in performance) and rationality (getting the wrong labe
 l is not better than getting no label at all) properties.  These conditio
 ns hold widely across many standard architectures. We complement this resu
 lt with an empirical study\, demonstrating that the generalization gap is 
 in fact small in practice and our bound is non-vacuous for many popular re
 presentation-learning based classifiers on CIFAR-10 and ImageNet\, includi
 ng SimCLR\, AMDIM and BigBiGAN.\n\nThe talk will not assume any specific b
 ackground in machine learning\, and should be accessible to a general math
 ematical audience. Joint work with Yamini Bansal and Gal Kaplun.\n\n.\n\n\
 nhttps://yins.yale.edu/event/yins-seminar-boaz-barak-harvard-university
LOCATION:Zoom
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-boaz-barak-harvard-university
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Daniel Roy (University of Toronto)
DTSTART;VALUE=DATE-TIME:20201118T170000
DTEND;VALUE=DATE-TIME:20201118T180000
DESCRIPTION:Event description:\n“In Defense of Uniform Convergence: Gene
 ralization via derandomization with an application to interpolating predic
 tors”\n\nSpeaker: Daniel Roy\n\nAssociate Professor\, University of Toro
 nto\n\nTo participate:\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\nht
 tps://yale.zoom.us/j/95827054840\n\nOr Telephone：203-432-9666 (2-ZOOM if
  on-campus) or 646 568 7788\nMeeting ID: 958 2705 4840\nInternational numb
 ers available:\nhttps://yale.zoom.us/u/aciY0peggr\n\nTalk Summary:\nWe pro
 pose to study the generalization error of a learned predictor ^h in terms 
 of that of a surrogate (potentially randomized) predictor that is coupled 
 to ^h and designed to trade empirical risk for control of generalization e
 rror. In the case where ^h interpolates the data\, it is interesting to co
 nsider theoretical surrogate classifiers that are partially derandomized o
 r rerandomized\, e.g.\, fit to the training data but with modified label n
 oise. We also show that replacing ^h by its conditional distribution with 
 respect to an arbitrary σ-field is a convenient way to derandomize. We st
 udy two examples\, inspired by the work of Nagarajan and Kolter (2019) and
  Bartlett et al. (2019)\, where the learned classifier ^h interpolates the
  training data with high probability\, has small risk\, and\, yet\, does n
 ot belong to a nonrandom class with a tight uniform bound on two-sided gen
 eralization error. At the same time\, we bound the risk of ^h in terms of 
 surrogates constructed by conditioning and denoising\, respectively\, and 
 shown to belong to nonrandom classes with uniformly small generalization e
 rror.\n\nJoint work by Jeffrey Negrea\, Gintare Karolina Dziugaite\, Danie
 l M. Roy\n\n.\n\n.\n\n\nhttps://yins.yale.edu/event/yins-seminar-daniel-ro
 y-university-toronto
LOCATION:Zoom
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-daniel-roy-university-toronto
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Distinguished Lecturer Seminar: Peter Bartlett (UC Berkeley)
DTSTART;VALUE=DATE-TIME:20201202T170000
DTEND;VALUE=DATE-TIME:20201202T180000
DESCRIPTION:Event description:\n“Benign Overfitting”\n\nSpeaker: Peter
  Bartlett\n\nProfessor of Computer Science and Statistics\, University of 
 California at Berkeley\nAssociate Director of the Simons Institute for the
  Theory of Computing\nDirector of the Foundations of Data Science Institut
 e\nDirector of the Collaboration on the Theoretical Foundations of Deep Le
 arning\n\nTo participate:\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\
 nhttps://yale.zoom.us/j/97135219127\nOr Telephone：203-432-9666 (2-ZOOM i
 f on-campus) or 646 568 7788\nMeeting ID: 971 3521 9127\nInternational num
 bers available:\nhttps://yale.zoom.us/u/abxwXKgpCp\n\nTalk Summary:\nClass
 ical theory that guides the design of nonparametric prediction methods lik
 e deep neural networks involves a tradeoff between the fit to the training
  data and the complexity of the prediction rule. Deep learning seems to op
 erate outside the regime where these results are informative\, since deep 
 networks can perform well even with a perfect fit to noisy training data. 
 We investigate this phenomenon of ‘benign overfitting’ in the simplest
  setting\, that of linear prediction. We give a characterization of linear
  regression problems for which the minimum norm interpolating prediction r
 ule has near-optimal prediction accuracy. The characterization is in terms
  of two notions of effective rank of the data covariance. It shows that ov
 erparameterization is essential: the number of directions in parameter spa
 ce that are unimportant for prediction must significantly exceed the sampl
 e size.  It also shows an important role for finite-dimensional data: ben
 ign overfitting occurs for a much narrower range of properties of the data
  distribution when the data lies in an infinite dimensional space versus w
 hen it lies in a finite dimensional space whose dimension grows faster tha
 n the sample size. We discuss implications for deep networks\, for robustn
 ess to adversarial examples\, and for the rich variety of possible behavio
 rs of excess risk as a function of dimension\, and we describe extensions 
 to ridge regression and barriers to analyzing benign overfitting based on 
 model-dependent generalization bounds.  Joint work with Phil Long\, Gábo
 r Lugosi\, and Alex Tsigler.\n\nSpeaker bio:\nPeter Bartlett is professor 
 of Computer Science and Statistics at the University of California at Berk
 eley\, Associate Director of the Simons Institute for the Theory of Comput
 ing\, Director of the Foundations of Data Science Institute\, and Director
  of the Collaboration on the Theoretical Foundations of Deep Learning. His
  research interests include machine learning and statistical learning theo
 ry\, and he is the co-author of the book Neural Network Learning: Theoreti
 cal Foundations. He has been Institute of Mathematical Statistics Medallio
 n Lecturer\, winner of the Malcolm McIntosh Prize for Physical Scientist o
 f the Year\, and Australian Laureate Fellow\, and he is a Fellow of the IM
 S\, Fellow of the ACM\, and Fellow of the Australian Academy of Science.\n
 \n.\n\n\nhttps://yins.yale.edu/event/yins-distinguished-lecturer-seminar-p
 eter-bartlett-uc-berkeley
LOCATION:Join from PC\, Mac\, Linux\, iOS or Android: https://yale.zoom.us
 /j/97135219127
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-distinguished-lecturer-seminar-peter-
 bartlett-uc-berkeley
END:VEVENT
END:VCALENDAR
