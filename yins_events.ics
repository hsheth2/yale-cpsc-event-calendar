BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS)
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Seminar: Marinka Zitnik (Harvard)
DTSTART;VALUE=DATE-TIME:20211103T160000
DTEND;VALUE=DATE-TIME:20211103T170000
DESCRIPTION:Event description:\nYINS Seminar:\n“\nInfusing Structure and
  Knowledge into Biomedical AI Algorithms”\n\nSpeaker: Marinka Zitnik\n\
 n\nAssistant Professor of Biomedical Informatics\, Harvard Medical School\
 nZitnik Lab:\nhttps://zitniklab.hms.harvard.edu\n\n\nTalk summary:\nGrand 
 challenges in biology and medicine often lack annotated examples and requi
 re generalization to entirely new scenarios not seen during training. Howe
 ver\, standard supervised learning is incredibly limited in scenarios\, su
 ch as designing novel medicines\, modeling emerging pathogens\, and treati
 ng rare diseases. In this talk\, I present our efforts to overcome these o
 bstacles by infusing structure and knowledge into learning algorithms. Fir
 st\, I outline our subgraph neural networks that can disentangle distinct 
 aspects of subgraph topology. I then present a general-purpose approach fo
 r few-shot learning on graphs. At the core is the notion of local subgraph
 s that transfer knowledge from one task to another\, even when only a hand
 ful of labeled examples are available. This principle is theoretically jus
 tified as we show the evidence for predictions can be found in subgraphs s
 urrounding the targets. I conclude with applications in drug development a
 nd precision medicine where the algorithmic predictions were validated in 
 human cells and led to the discovery of a new class of drugs.\n\nTo partic
 ipate:\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.
 us/j/95292478213\nOr Telephone：203-432-9666 (2-ZOOM if on-campus) or 646
  568 7788\nMeeting ID: 952 9247 8213\nInternational numbers available:\nht
 tps://yale.zoom.us/u/abCoS8cam8\n\n\nSpeaker bio:\nMarinka Zitnik is an As
 sistant Professor at Harvard University with appointments in the Departmen
 t of Biomedical Informatics\, Broad Institute of MIT and Harvard\, and Har
 vard Data Science. Dr. Zitnik leads the Machine learning for Medicine and 
 Science Lab\, focusing on methods and applications for networked systems t
 hat require infusing structure and domain knowledge. This research won bes
 t paper and research awards from the International Society for Computation
 al Biology\, International Conference of Machine Learning\, Bayer Early Ex
 cellence in Science Award\, Amazon Faculty Research Award\, Rising Star Aw
 ard in EECS\, and Next Generation Recognition in Biomedicine\, being the o
 nly young scientist who received such recognition in both EECS and Biomedi
 cine.\n\n.\n\n\nhttps://yins.yale.edu/event/yins-seminar-marinka-zitnik-ha
 rvard
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-marinka-zitnik-harvard
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Karan Singh (Princeton)
DTSTART;VALUE=DATE-TIME:20211110T170000
DTEND;VALUE=DATE-TIME:20211110T180000
DESCRIPTION:Event description:\nYINS Seminar\,\n“\nBoosting for Online C
 onvex Optimization”\n\nSpeaker: Karan Singh\n\nPostdoc\, Princeton Unive
 rsity\n\nTalk summary:\nBoosting is a computational framework for composit
 ional learning. There are two traditions to the theory of boosting. First:
  Classical boosting\, arising from theoretical CS\, converts weak slightly
 -better-than-random learners to an accurate one\, enhancing the accuracy. 
 Originally designed for the binary classification setting\, the literature
  on boosting was extended to multi-class\, multi-label\, and ranking-based
  settings (all examples of linear loss) with specialized constructions in 
 each case. Second: Gradient boosting\, on the other hand\, aggregates simp
 le (but accurate) learners into a more expressive one\; it guarantees comp
 etitiveness with the convex hull of the weak hypothesis class.  The main 
 contribution of this work is an efficient algorithm that enhances the accu
 racy and expressivity of the learning process at the same time\, while ope
 rating on general convex loss (vs. linear for classical boosting) and any 
 convex decision set. This resultant excess risk (average regret) guarantee
  unifies and delivers on the twin objectives of classical boosting and gra
 dient boosting. The reduction holds for both the (non-stochastic) online a
 nd statistical settings\, and is amenable to bandit feedback.\n\nTo partic
 ipate:\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.
 us/j/98411487580\nOr Telephone：203-432-9666 (2-ZOOM if on-campus) or 646
  568 7788\nMeeting ID: 984 1148 7580\nInternational numbers available:\nht
 tps://yale.zoom.us/u/aypRvb6I6\n\nSpeaker bio:\nKaran Singh is a postdocto
 ral researcher at Microsoft Research in Redmond. He received a PhD in Comp
 uter Science\, under the supervision of Prof. Elad Hazan\, from Princeton 
 University in 2021. His research looks at questions in supervised and inte
 ractive learning through the lens of optimization. This has\, in recent ye
 ars\, shaped into a quest towards an algorithmic (vs. traditionally\, anal
 ytic) foundation for control theory\, for learning in dynamical systems th
 at do not “forget”\, and for provably sound mechanisms of compositiona
 l learning.\n\n.\n\n\nhttps://yins.yale.edu/event/yins-seminar-karan-singh
 -princeton
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-karan-singh-princeton
END:VEVENT
END:VCALENDAR
