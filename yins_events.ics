BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS)
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Seminar: Dorsa Sadigh (Stanford)
DTSTART;VALUE=DATE-TIME:20210519T160000
DTEND;VALUE=DATE-TIME:20210519T170000
DESCRIPTION:Event description:\n“Walking the Boundary of Learning and In
 teraction”\n\nSpeaker: Dorsa Sadigh\n\nAssistant Professor\, Computer Sc
 ience and Electrical Engineering\nStanford University\n\nTalk summary:\nTh
 ere have been significant advances in the field of robot learning in the p
 ast decade. However\, many challenges still remain when considering how ro
 bot learning can advance interactive agents such as robots that collaborat
 e with humans\, and how interactions can enable more effective robot learn
 ing. This introduces an opportunity for developing new robot learning algo
 rithms that can help advance interactive autonomy. In this talk\, I will d
 iscuss a formalism for human-robot interaction built upon ideas from repre
 sentation learning. This formalism provides an orthogonal perspective to t
 heory of mind\, and provides a path for scalable partner modeling. Specifi
 cally\, I will first discuss the notion of latent strategies — low dimen
 sional representations sufficient for capturing non-stationary interaction
 s. I will then talk about some of the challenges of learning such represen
 tations when interacting with humans\, and how we can develop data-efficie
 nt techniques that enable actively learning computational models of human 
 behavior from interaction data: demonstrations\, preferences\, or physical
  corrections. Finally\, I will wrap up by discussing some of the challenge
 s that arise when considering long-term repeated interactions\, and how pa
 rtner-specific conventions can be leveraged for fast adaptation on new col
 laborative tasks.\n\nTo participate:\n\nJoin from PC\, Mac\, Linux\, iOS o
 r Android:\nhttps://yale.zoom.us/j/99603813308\nOr Telephone：203-432-966
 6 (2-ZOOM if on-campus) or 646 568 7788\nMeeting ID: 996 0381 3308\nIntern
 ational numbers available:\nhttps://yale.zoom.us/u/ackR7i1KJw\n\nSpeaker B
 io:\nDorsa Sadigh is an assistant professor in Computer Science and Electr
 ical Engineering at Stanford University.  Her research interests lie in t
 he intersection of robotics\, learning\, and control theory. Specifically\
 , she is interested in developing algorithms for safe and adaptive human-r
 obot interaction. Dorsa has received her doctoral degree in Electrical Eng
 ineering and Computer Sciences (EECS) from UC Berkeley in 2017\, and has r
 eceived her bachelor’s degree in EECS from UC Berkeley in 2012.  She is
  awarded the NSF CAREER award\, the AFOSR Young Investigator award\, the I
 EEE TCCPS early career award\, the Google Faculty Award\, and the Amazon F
 aculty Research Award.\n\n.\n\n\nhttps://yins.yale.edu/event/yins-seminar-
 dorsa-sadigh-stanford
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-dorsa-sadigh-stanford
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Distinguished Lecturer: Robert Nowak (Univ of Wisconsin-Madis
 on)
DTSTART;VALUE=DATE-TIME:20210526T160000
DTEND;VALUE=DATE-TIME:20210526T170000
DESCRIPTION:Event description:\nYINS Distinguished Lecturer\n\n“Banach S
 pace Representer Theorems for Neural Networks”\n\nSpeaker: Robert D. Now
 ak\n\nNosbusch Professor of Engineering\, University of Wisconsin-Madison\
 n\nTalk summary:\nThis talk presents a variational framework to understand
  the properties of functions learned by neural networks fit to data. The f
 ramework is based on total variation semi-norms defined in the Radon domai
 n\, which is naturally suited to the analysis of neural activation functio
 ns (ridge functions). Finding a function that fits a dataset while having 
 a small semi-norm is posed as an infinite dimensional variational optimiza
 tion.  We derive a representer theorem showing that finite-width neural n
 etworks are solutions to the variational problem. The representer theorem 
 is reminiscent of the classical reproducing kernel Hilbert space represent
 er theorem\, but we show that neural networks are solutions in a non-Hilbe
 rtian Banach space. While the learning problems are posed in an infinite d
 imensional function space\, similar to kernel methods\, they can be recast
  as finite-dimensional neural network training problems. These neural netw
 ork training problems have regularizers which are related to the well-know
 n weight decay and path-norm regularizers. Thus\, the results provide new 
 insight into functional characteristics of overparameterized neural networ
 ks and also into the design neural network regularizers.  Our results als
 o provide new theoretical support for a number of empirical findings in de
 ep learning architectures including the benefits of “skip connections”
 \, sparsity\, and low-rank structures.\n\nThis is joint work with Rahul Pa
 rhi.\n\nTo participate:\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhtt
 ps://yale.zoom.us/j/98536270943\nOr Telephone：203-432-9666 (2-ZOOM if on
 -campus) or 646 568 7788\nMeeting ID: 985 3627 0943\nInternational numbers
  available:\nhttps://yale.zoom.us/u/adQwraHGOC\n\n\nSpeaker bio:\nRobert N
 owak holds the Nosbusch Professorship in Engineering at the University of 
 Wisconsin-Madison\, where his research focuses on signal processing\, mach
 ine learning\, optimization\, and statistics.\n\n.\n\n\nhttps://yins.yale.
 edu/event/yins-distinguished-lecturer-robert-nowak-univ-wisconsin-madison
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-distinguished-lecturer-robert-nowak-u
 niv-wisconsin-madison
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Csaba Szepesvari (Univ of Alberta)
DTSTART;VALUE=DATE-TIME:20210602T160000
DTEND;VALUE=DATE-TIME:20210602T170000
DESCRIPTION:Event description:\nYINS Seminar\n\nSpeaker: Csaba Szepesvari
 \n\nProfessor & Canada CIFAR AI Chair\, Amii\nDepartment of Computing Scie
 nce\nUniversity of Alberta\n\nTo participate:\n\nJoin from PC\, Mac\, Linu
 x\, iOS or Android:\nhttps://yale.zoom.us/j/94695053313\nOr Telephone：20
 3-432-9666 (2-ZOOM if on-campus) or 646 568 7788\nMeeting ID: 946 9505 331
 3\nInternational numbers available:\nhttps://yale.zoom.us/u/at3KTCmpZ\n\nS
 peaker bio:\n\nhttps://sites.ualberta.ca/~szepesva/\n\n\n.\n\n\nhttps://yi
 ns.yale.edu/event/yins-seminar-csaba-szepesvari-univ-alberta
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-csaba-szepesvari-univ-alberta
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Alum Summer Seminar: Rasmus Kyng (ETH Zurich)
DTSTART;VALUE=DATE-TIME:20210616T160000
DTEND;VALUE=DATE-TIME:20210616T170000
DESCRIPTION:Event description:\nYINS Alum Summer Seminar\n\nSpeaker:\nRasm
 us Kyng\n\nAssistant Professor\, ETH Zurich\n\nTo participate:\n\nJoin fro
 m PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.us/j/97920219362\n
 Or Telephone：203-432-9666 (2-ZOOM if on-campus) or 646 568 7788\nMeeting
  ID: 979 2021 9362\nInternational numbers available:\nhttps://yale.zoom.us
 /u/afjMjLJlB\n\n.\n\n\nhttps://yins.yale.edu/event/yins-alum-summer-semina
 r-rasmus-kyng-eth-zurich
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-alum-summer-seminar-rasmus-kyng-eth-z
 urich
END:VEVENT
BEGIN:VEVENT
SUMMARY:"Recovering tree models via spectral graph theory" with speakers Y
 ariv Aizenbud and Ariel Jaffe (Yale Math)
DTSTART;VALUE=DATE-TIME:20210514T170000
DTEND;VALUE=DATE-TIME:20210514T180000
DESCRIPTION:Event description:\n“Recovering tree models via spectral gra
 ph theory”\n\nSpeakers: Yariv Aizenbud and Ariel Jaffe (Yale Math)\n\nAb
 stract:\nModeling data by latent tree models is a powerful approach in mul
 tiple machine learning applications (e.g. speech analysis and bioinformati
 cs). A canonical example of this setting is the “tree of life”\, where
  the evolutionary history of a set of organisms is inferred by their DNA. 
 Generally\, in latent tree models\, the main task is to infer the structur
 e of the tree\, given only observations of its terminal nodes. While infer
 ring a tree structure is a common task\, in many applications\, a robust a
 lgorithm for the recovery of large trees is still missing.\n\nIn this talk
 \, we will see a new method for the recovery of latent tree models\, which
  is based on spectral graph theory. We show that the hidden tree structure
  is strongly related to the spectral properties of a fully connected graph
 \, defined over the terminal nodes of the tree. Finally\, we see that whil
 e in terms of accuracy the method performs similarly to state-of-the-art m
 ethods\, it is significantly more computationally efficient.\n\nTo partici
 pate:\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.u
 s/j/98218919780\n\n.\n\n\nhttps://yins.yale.edu/event/recovering-tree-mode
 ls-spectral-graph-theory-speakers-yariv-aizenbud-and-ariel-jaffe-yale-math
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/recovering-tree-models-spectral-graph-theo
 ry-speakers-yariv-aizenbud-and-ariel-jaffe-yale-math
END:VEVENT
END:VCALENDAR
