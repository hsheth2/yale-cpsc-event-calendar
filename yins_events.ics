BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS)
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Distinguished Lecturer Seminar: Peter Bartlett (UC Berkeley)
DTSTART;VALUE=DATE-TIME:20201202T170000
DTEND;VALUE=DATE-TIME:20201202T180000
DESCRIPTION:Event description:\n“Benign Overfitting”\n\nSpeaker: Peter
  Bartlett\n\nProfessor of Computer Science and Statistics\, University of 
 California at Berkeley\nAssociate Director of the Simons Institute for the
  Theory of Computing\nDirector of the Foundations of Data Science Institut
 e\nDirector of the Collaboration on the Theoretical Foundations of Deep Le
 arning\n\nTo participate:\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\
 nhttps://yale.zoom.us/j/97135219127\nOr Telephone：203-432-9666 (2-ZOOM i
 f on-campus) or 646 568 7788\nMeeting ID: 971 3521 9127\nInternational num
 bers available:\nhttps://yale.zoom.us/u/abxwXKgpCp\n\nTalk Summary:\nClass
 ical theory that guides the design of nonparametric prediction methods lik
 e deep neural networks involves a tradeoff between the fit to the training
  data and the complexity of the prediction rule. Deep learning seems to op
 erate outside the regime where these results are informative\, since deep 
 networks can perform well even with a perfect fit to noisy training data. 
 We investigate this phenomenon of ‘benign overfitting’ in the simplest
  setting\, that of linear prediction. We give a characterization of linear
  regression problems for which the minimum norm interpolating prediction r
 ule has near-optimal prediction accuracy. The characterization is in terms
  of two notions of effective rank of the data covariance. It shows that ov
 erparameterization is essential: the number of directions in parameter spa
 ce that are unimportant for prediction must significantly exceed the sampl
 e size.  It also shows an important role for finite-dimensional data: ben
 ign overfitting occurs for a much narrower range of properties of the data
  distribution when the data lies in an infinite dimensional space versus w
 hen it lies in a finite dimensional space whose dimension grows faster tha
 n the sample size. We discuss implications for deep networks\, for robustn
 ess to adversarial examples\, and for the rich variety of possible behavio
 rs of excess risk as a function of dimension\, and we describe extensions 
 to ridge regression and barriers to analyzing benign overfitting based on 
 model-dependent generalization bounds.  Joint work with Phil Long\, Gábo
 r Lugosi\, and Alex Tsigler.\n\nSpeaker bio:\nPeter Bartlett is professor 
 of Computer Science and Statistics at the University of California at Berk
 eley\, Associate Director of the Simons Institute for the Theory of Comput
 ing\, Director of the Foundations of Data Science Institute\, and Director
  of the Collaboration on the Theoretical Foundations of Deep Learning. His
  research interests include machine learning and statistical learning theo
 ry\, and he is the co-author of the book Neural Network Learning: Theoreti
 cal Foundations. He has been Institute of Mathematical Statistics Medallio
 n Lecturer\, winner of the Malcolm McIntosh Prize for Physical Scientist o
 f the Year\, and Australian Laureate Fellow\, and he is a Fellow of the IM
 S\, Fellow of the ACM\, and Fellow of the Australian Academy of Science.\n
 \n.\n\n\nhttps://yins.yale.edu/event/yins-distinguished-lecturer-seminar-p
 eter-bartlett-uc-berkeley
LOCATION:Join from PC\, Mac\, Linux\, iOS or Android: https://yale.zoom.us
 /j/97135219127
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-distinguished-lecturer-seminar-peter-
 bartlett-uc-berkeley
END:VEVENT
END:VCALENDAR
