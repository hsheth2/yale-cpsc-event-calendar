BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS)
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Industry Seminar: Vitaly Feldman (Apple AI)
DTSTART;VALUE=DATE-TIME:20210915T040000
DTEND;VALUE=DATE-TIME:20210915T050000
DESCRIPTION:Event description:\nYINS Industry Seminar\n\n“Hiding Among t
 he Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification b
 y Shuffling”\n\nSpeaker: Vitaly Feldman\n\nResearch Scientist\, Apple AI
 \n\nTalk summary:\nRecent work of Erlingsson et al (2019) demonstrates tha
 t random shuffling amplifies differential privacy guarantees of locally ra
 ndomized data. Such amplification implies substantially stronger privacy g
 uarantees for systems in which data is contributed anonymously and has lea
 d to significant interest in the shuffle model of privacy.\n\n\n\nWe show 
 that random shuffling of $n$ data records that are input to $\\epsilon_0$-
 differentially private local randomizers results in an $(O((1-e^{-\\epsilo
 n_0})\\sqrt{\\frac{e^{\\epsilon_0}\\log(1/\\delta)}{n}})\, \\delta)$-diffe
 rentially private algorithm. This significantly improves over previous wor
 k and achieves the asymptotically optimal dependence in $\\epsilon_0$. Our
  result is based on a new approach that is simpler than previous work and 
 extends to approximate differential privacy with nearly the same guarantee
 s. Importantly\, our work also yields an algorithm for deriving tighter bo
 unds on the resulting $\\epsilon$ and $\\delta$ as well as Renyi different
 ial privacy guarantees. We show numerically that our algorithm gets to wit
 hin a small constant factor of the optimal bound. As a direct corollary of
  our analysis we derive a simple and asymptotically optimal algorithm for 
 discrete distribution estimation in the shuffle model of privacy. We also 
 observe that our result implies the first asymptotically optimal privacy a
 nalysis of noisy stochastic gradient descent that applies to sampling with
 out replacement.\n\n\n\nJoint work with Audra McMillan and Kunal Talwar\n\
 nTo participate:\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttps://ya
 le.zoom.us/j/95960621157\nOr Telephone：203-432-9666 (2-ZOOM if on-campus
 ) or 646 568 7788\nMeeting ID: 959 6062 1157\nInternational numbers availa
 ble:\nhttps://yale.zoom.us/u/acKNWHMbeA\n\n\n\n\n\n.\n\n\nhttps://yins.yal
 e.edu/event/yins-industry-seminar-vitaly-feldman-apple-ai
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-industry-seminar-vitaly-feldman-apple
 -ai
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Daniel Kuhn (EPFL)
DTSTART;VALUE=DATE-TIME:20210922T160000
DTEND;VALUE=DATE-TIME:20210922T170000
DESCRIPTION:Event description:\nYINS Seminar:\n“Wasserstein Distribution
 ally Robust Optimization: Theory and Applications in Machine Learning”\n
 \nSpeaker: Daniel Kuhn (EPFL)\n\nChair of Risk Analytics and Optimization 
 at EPFL\n\nAbstract:\nMany decision problems in science\, engineering and 
 economics are affected by uncertain parameters whose distribution is only 
 indirectly observable through samples. The goal of data-driven decision-ma
 king is to learn a decision from finitely many training samples that will 
 perform well on unseen test samples. This learning task is difficult even 
 if all training and test samples are drawn from the same distribution - es
 pecially if the dimension of the uncertainty is large relative to the trai
 ning sample size. Wasserstein distributionally robust optimization seeks d
 ata-driven decisions that perform well under the most adverse distribution
  within a certain Wasserstein distance from a nominal distribution constru
 cted from the training samples. In this talk we will see that this approac
 h has many conceptual and computational benefits. Most prominently\, the o
 ptimal decisions can often be computed by solving tractable convex optimiz
 ation problems\, and they enjoy rigorous out-of-sample and asymptotic cons
 istency guarantees. We will also show that Wasserstein distributionally ro
 bust optimization has interesting ramifications for statistical learning a
 nd motivates new approaches for fundamental learning tasks such as classif
 ication\, regression\, maximum likelihood estimation or minimum mean squar
 e error estimation\, among others.\n\nSpeaker bio:\nDaniel Kuhn holds the 
 Chair of Risk Analytics and Optimization at EPFL. Before joining EPFL\, he
  was a faculty member at Imperial College London (2007-2013) and a postdoc
 toral researcher at Stanford University (2005-2006). He received a PhD in 
 Economics from the University of St. Gallen in 2004 and an MSc in Theoreti
 cal Physics from ETH Zurich in 1999. His research interests revolve around
  robust optimization and stochastic programming.\n\nTo participate:\nJoin 
 from PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.us/j/9915754365
 3\nOr Telephone：203-432-9666 (2-ZOOM if on-campus) or 646 568 7788\nMeet
 ing ID: 991 5754 3653\nInternational numbers available:\nhttps://yale.zoom
 .us/u/abrK6kHkDy\n\n.\n\n\nhttps://yins.yale.edu/event/yins-seminar-daniel
 -kuhn-epfl
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-daniel-kuhn-epfl
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Daniel Hsu (Columbia)
DTSTART;VALUE=DATE-TIME:20211006T160000
DTEND;VALUE=DATE-TIME:20211006T170000
DESCRIPTION:Event description:\nYINS Seminar\n\n“Contrastive learning\, 
 multi-view redundancy\, and linear models”\n\nSpeaker: Daniel Hsu\n\nCol
 umbia University\n\nAbstract:\nContrastive learning is a “self-supervise
 d” approach to representation learning that uses naturally occurring sim
 ilar and dissimilar pairs of data points to find useful embeddings of data
 . We study contrastive learning in the context of multi-view statistical m
 odels. First\, we show that whenever the views of the data are approximate
 ly redundant in their ability to predict a target function\, a low-dimensi
 onal embedding obtained via contrastive learning affords a linear predicto
 r with near-optimal predictive accuracy. Second\, we show that in the cont
 ext of topic models\, the embedding can be interpreted as a linear transfo
 rmation of the posterior moments of the hidden topic distribution given th
 e observed words. We also empirically demonstrate that linear classifiers 
 with these representations perform well in document classification tasks w
 ith very few labeled examples in a semi-supervised setting.\n\nThis is joi
 nt work with Akshay Krishnamurthy (MSR) and Christopher Tosh (Columbia).\n
 \nSpeaker Bio:\nDaniel Hsu is an associate professor in the Department of 
 Computer Science and a member of the Data Science Institute\, both at Colu
 mbia University. Previously\, he was a postdoc at Microsoft Research New E
 ngland\, and the Departments of Statistics at Rutgers University and the U
 niversity of Pennsylvania. He holds a Ph.D. in Computer Science from UC Sa
 n Diego\, and a B.S. in Computer Science and Engineering from UC Berkeley.
  He was selected by IEEE Intelligent Systems as one of “AI’s 10 to Wat
 ch” in 2015 and received a 2016 Sloan Research Fellowship. Daniel’s re
 search interests are in algorithmic statistics and machine learning.\n\nTo
  participate:\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.
 zoom.us/j/93949928143\nOr Telephone：203-432-9666 (2-ZOOM if on-campus) o
 r 646 568 7788\nMeeting ID: 939 4992 8143\nInternational numbers available
 :\nhttps://yale.zoom.us/u/af5ZYLPKB\n\n.\n\n\nhttps://yins.yale.edu/event/
 yins-seminar-daniel-hsu-columbia
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-daniel-hsu-columbia
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Steve Hanneke (TTIC)
DTSTART;VALUE=DATE-TIME:20211013T160000
DTEND;VALUE=DATE-TIME:20211013T170000
DESCRIPTION:Event description:\nYINS Seminar\n\nSpeaker: Steve Hanneke\n\n
 Research Assistant Professor\, Toyota Technological Institute at Chicago\n
 \nTo participate:\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttps://y
 ale.zoom.us/j/96519593299\nOr Telephone：203-432-9666 (2-ZOOM if on-campu
 s) or 646 568 7788\nMeeting ID: 965 1959 3299\nInternational numbers avail
 able:\nhttps://yale.zoom.us/u/acN1RQemY6\n\nSpeaker bio:\n\nSteve Hanneke 
 earned a Bachelor of Science degree in Computer Science from UIUC in 2005 
 and a Ph.D. in Machine Learning from Carnegie Mellon University in 2009 wi
 th a dissertation on the theoretical foundations of active learning. He wa
 s a Visiting Assistant Professor in the Department of Statistics at Carneg
 ie Mellon University from 2009-2012\, conducted research as an independent
  scientist from 2012-2018\, and was a Visiting Lecturer at Princeton Unive
 rsity in spring 2018. He joined TTIC as a Research Assistant Professor in 
 fall 2018.\n\nSteve’s research explores the theory of machine learning: 
 designing new learning algorithms capable of learning from fewer samples\,
  understanding the benefits and capabilities of interactive machine learni
 ng\, developing new perspectives on transfer learning and life-long learni
 ng\, and revisiting the basic probabilistic assumptions at the foundation 
 of learning theory.\n\nDr. Hanneke also has a personally maintained websit
 e which can be found at\nhttp://www.ttic.edu/hanneke\n\n.\n\n\nhttps://yin
 s.yale.edu/event/yins-seminar-steve-hanneke-ttic
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-steve-hanneke-ttic
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Avrim Blum (TTIC)
DTSTART;VALUE=DATE-TIME:20211020T160000
DTEND;VALUE=DATE-TIME:20211020T170000
DESCRIPTION:Event description:\nYINS Seminar\n\nSpeaker: Avrim Blum\n\nPro
 fessor and Chief Academic Officer\nToyota Technological Institute at Chica
 go\n\nhttp://www.ttic.edu/blum\n\nTo participate:\nJoin from PC\, Mac\, Li
 nux\, iOS or Android:\nhttps://yale.zoom.us/j/93612337599\nOr Telephone：
 203-432-9666 (2-ZOOM if on-campus) or 646 568 7788\nMeeting ID: 936 1233 7
 599\nInternational numbers available:\nhttps://yale.zoom.us/u/abMfmltZ0O\n
 \n.\n\n\nhttps://yins.yale.edu/event/yins-seminar-avrim-blum-ttic
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-avrim-blum-ttic
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Industry Seminar: Yasaman Bahri (Google)
DTSTART;VALUE=DATE-TIME:20211027T160000
DTEND;VALUE=DATE-TIME:20211027T170000
DESCRIPTION:Event description:\nYINS Industry Seminar\n\nSpeaker: Yasaman 
 Bahri\n\nResearch Scientist at Google Research\, Brain Team\n\nTo particip
 ate:\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.us/j
 /98819118892\nOr Telephone：203-432-9666 (2-ZOOM if on-campus) or 646 568
  7788\nMeeting ID: 988 1911 8892\nInternational numbers available:\nhttps:
 //yale.zoom.us/u/aeF0HRmJcA\n\n\n.\n\n\nhttps://yins.yale.edu/event/yins-i
 ndustry-seminar-yasaman-bahri-google
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-industry-seminar-yasaman-bahri-google
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Marinka Zitnik (Harvard)
DTSTART;VALUE=DATE-TIME:20211103T160000
DTEND;VALUE=DATE-TIME:20211103T170000
DESCRIPTION:Event description:\nYINS Seminar\n\nSpeaker: Marinka Zitnik\n
 \n\nAssistant Professor of Biomedical Informatics\, Harvard Medical School
 \nZitnik Lab:\nhttps://zitniklab.hms.harvard.edu\n\n\nTo participate:\nJoi
 n from PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.us/j/95292478
 213\nOr Telephone：203-432-9666 (2-ZOOM if on-campus) or 646 568 7788\nMe
 eting ID: 952 9247 8213\nInternational numbers available:\nhttps://yale.zo
 om.us/u/abCoS8cam8\n\n\n.\n\n\nhttps://yins.yale.edu/event/yins-seminar-ma
 rinka-zitnik-harvard
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-marinka-zitnik-harvard
END:VEVENT
END:VCALENDAR
