BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS) Events Calendar
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Distinguished Lecturer: Alexander (Sasha) Rakhlin (MIT)
DTSTART;VALUE=DATE-TIME:20200304T170000
DTEND;VALUE=DATE-TIME:20200304T180000
DESCRIPTION:Event description:\n“Is Memorization Compatible with Learnin
 g?”\n\nSpeaker:\nAlexander (Sasha) Rakhlin\, Associate Professor\, Massa
 chusetts Institute of Technology\nCenter for Statistics\, IDSS\, Departmen
 t of Brain & Cognitive Sciences\, Laboratory for Information & Decision Sy
 stems\, Center for Brains\, Minds\, and Machines\n\n\nAbstract:\nOne of th
 e key tenets taught in courses on Statistics and Machine Learning is that 
 fitting the data too well inevitably leads to overfitting and poor predict
 ion performance. Yet\, over-parametrized neural networks appear to defy th
 is “rule”. We will provide theoretical evidence that challenges the co
 mmon wisdom. In particular\, we will consider the minimum norm interpolant
  in a reproducing kernel Hilbert space and show its good generalization pr
 operties in certain high-dimensional regimes. Furthermore\, our estimates 
 suggest a counterintuitive “multiple descent” phenomenon whereby more 
 data leads to alternating phases of better and worse performance.\n\n\n\nS
 ince gradient dynamics for wide randomly-initialized neural networks prova
 bly converge to a minimum-norm interpolant (with respect to a certain kern
 el)\, our results imply generalization and consistency for such neural net
 works. We will contrast our approach with the classical techniques based o
 n uniform convergence and Rademacher averages and argue that these techniq
 ues are not sufficient for analyzing the memorization regime.\n\n\n\nJoint
  work with Tengyuan Liang and Xiyu Zhai.\n\n\n\n\n\nSpeaker Bio:\nAlexande
 r (Sasha) Rakhlin is an Associate Professor at MIT. His research is in Sta
 tistics and Machine Learning. He received his bachelor’s degrees in math
 ematics and computer science from Cornell University\, and doctoral degree
  in computational neuroscience from MIT. He was a postdoc at UC Berkeley E
 ECS before joining the University of Pennsylvania\, where he was an Associ
 ate Professor in the Department of Statistics before joining MIT.\nhttp://
 www.mit.edu/~rakhlin/\n\n\n\n\n\n\nhttps://yins.yale.edu/event/yins-distin
 guished-lecturer-alexander-sasha-rakhlin-mit
LOCATION:Yale Institute for Network Science
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-distinguished-lecturer-alexander-sash
 a-rakhlin-mit
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Distinguished Lecturer: Christos Papadimitriou (Columbia)
DTSTART;VALUE=DATE-TIME:20200401T160000
DTEND;VALUE=DATE-TIME:20200401T170000
DESCRIPTION:Event description:\n“Language\, Brain\, and Computation”\n
 \nSpeaker: Christos H. Papadimitriou\nProfessor of Computer Science\, Col
 umbia University\n\nAbstract:\nComputation in the brain has been modeled p
 roductively at many scales\, ranging from molecules to dendrites\, neurons
  and synapses\, all the way to the whole brain models useful in cognitive 
 science.  I will discuss recent work on anl intermediate computational la
 yer\, involving assemblies of neurons — that is to say\, populations of 
 neurons firing together in a repetitive pattern whenever we think of a par
 ticular memory\, concept or idea.  Assemblies were conjectured seven deca
 des ago by Hebb\, and have been over the past two decades noticed in both 
 the animal and the human brain.  Further\, experiments\, simulations\, an
 d theoretical analysis suggest that assemblies can be copied from one brai
 n area to another\, can be associated with other assemblies\, and more.  
 We propose a broader “calculus” of assemblies\, including the operatio
 n “merge”\, comprising a powerful computational model.  One interesti
 ng hypothesis is that assembly operations may underlie some of the most ad
 vanced functions of the human brain\, especially language.  Joint work wi
 th Santosh Vempala\, Wolfgang Maass\, and Michael Collins.\n\n\n\nSpeaker 
 Bio:\nChristos H. Papadimitriou is the Donovan Family professor of compute
 r science at Columbia University.  Before joining Columbia in 2017\, he t
 aught at UC Berkeley for 22 years\, and before that at Harvard\, MIT\, NTU
  Athens\, Stanford\, and UCSD.  He has written five textbooks and many ar
 ticles on algorithms and complexity\, and their applications to optimizati
 on\, databases\, control\, AI\, robotics\, economics and game theory\, the
  Internet\, evolution\, and more recently the study of the brain.  He hol
 ds a PhD from Princeton as well as eight honorary doctorates\, and he has 
 won the Knuth prize\, the Goedel prize\, the von Neumann medal\, and most 
 recently Technion’s Harvey prize.  He is a member of the National Acade
 my of Sciences of the US\, the American Academy of Arts and Sciences\, and
  the National Academy of Engineering\, while in 2013 the president of Gree
 ce named him commander of the order of the phoenix.  He has also written 
 three novels: “Turing”\, “Logicomix” and his latest “Independenc
 e”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n\nhttps://yins.yale.
 edu/event/yins-distinguished-lecturer-christos-papadimitriou-columbia
LOCATION:Yale Institute for Network Science
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-distinguished-lecturer-christos-papad
 imitriou-columbia
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS / CS Seminar: Using Natural Language Explanations to Incorpor
 ate Commonsense Reasoning in Neural Networks with Nazneen Rajani (Salesfor
 ce Research)
DTSTART;VALUE=DATE-TIME:20200303T200000
DTEND;VALUE=DATE-TIME:20200303T210000
DESCRIPTION:Event description:\nSpeaker: Nazneen Rajani\,\nSalesforce Rese
 arch\n\nDescription:\nDeep learning models perform poorly on tasks that re
 quire commonsense reasoning\, which often necessitates some form of world 
 knowledge or reasoning over information not immediately present in the inp
 ut. In the first part of the talk\, I will discuss how language models can
  be leveraged to generate natural language explanations which are not just
  interpretable but can also be used to improve performance on a downstream
  task such as CommonsenseQA and empirically show that explanations are a w
 ay to incorporate commonsense reasoning in neural networks. Further\, I wi
 ll discuss how explanations can be transferred to other tasks without fine
 -tuning. In the second part of the talk\, I will talk about how we can tra
 in neural networks to do commonsense reasoning for qualitative physics and
  demonstrate on simulations involving physical laws such as collision\, fr
 iction\, and gravity. Our proposed framework first detects salient collisi
 ons and then generates natural language reasoning about the events in thos
 e salient frames.\n\nSpeaker Bio:\nI am a Research Scientist at Salesforce
  Research since January 2019. My primary research interests are in the fie
 lds of Natural Language Understanding\, Machine Learning and Explainable A
 rtificial Intelligence (XAI). I am currently working on projects in the ar
 eas of language modeling with commonsense reasoning\, language grounding w
 ith vision\, gender bias in language modeling and explainable reinforcemen
 t learning. Website:\nhttp://www.nazneenrajani.com/\n\nClick here for the 
 Computer Science Event Posting\n\n\n\n\n\n\n\nhttps://yins.yale.edu/event/
 yins-cs-seminar-using-natural-language-explanations-incorporate-commonsens
 e-reasoning-neural
LOCATION:Yale Institute for Network Science
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-cs-seminar-using-natural-language-exp
 lanations-incorporate-commonsense-reasoning-neural
END:VEVENT
END:VCALENDAR
