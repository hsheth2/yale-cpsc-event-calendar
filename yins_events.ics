BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS)
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Seminar: Vardis Kandiros (MIT)
DTSTART;VALUE=DATE-TIME:20221102T160000
DTEND;VALUE=DATE-TIME:20221102T170000
DESCRIPTION:Event description:\nYINS Seminar: Vardis Kandiros\n\n“Learni
 ng Ising Models from One or Multiple Samples”\n\nSpeaker: Vardis Kandiro
 s\nMIT\n\nAbstract:\nSamples from high-dimensional distributions can be sc
 arce or expensive. Can we meaningfully learn such distributions from one o
 r just a few samples? We answer this question by studying the settings of 
 classification and regression when the input data consists of one vector\,
  with dependencies among it’s entries.   For classification\, the probl
 em can be cast as that of learning Ising models from a single sample. We p
 rovide a polynomial time algorithm for this task and quantify the estimati
 on error in terms of the metric entropy of possible interaction matrices. 
 As a Corollary of our result\, we can recover the network of dependencies 
 when the graph structure is low dimensional . Furthermore\, our result han
 dles multiple independent samples by viewing them as one sample from a lar
 ger model\, and can be used to derive estimation bounds that are qualitati
 vely similar to state-of-the-art in the multiple-sample literature. We thu
 s unify two separate strands of work in the literature\, one in Computer S
 cience on estimating Ising models/MRFs from multiple independent samples a
 nd one in Probability Theory on estimating them from one sample in restric
 tive settings. On the technical front\, we reduce our model to a sparsifie
 d version of it where only weak dependencies exist and exploit novel conce
 ntration and anti-concentration inequalities for functions of the Ising mo
 del in that regime. We also study the setting of logistic regression in th
 e presence of dependencies. This can again be formulated as an Ising model
  with two possibly multidimensional parameters: one regression parameter a
 nd one that controls the dependencies. Perhaps surprisingly\, we show that
  it might only be possible to learn only one of the two parameters. We cha
 racterize the optimal learning rates for each parameter and provide polyno
 mial time algorithms for achieving them. This requires proving approximati
 on guarantees for the naïve mean-field equation from statistical physics 
 and using it to obtain sharp bounds for the moments of the distribution. B
 ased on joint work with Yuval Dagan\, Constantinos Daskalakis\, Nishanth D
 ikkala and Surbhi Goel.​\n\nSpeaker Bio:\nVardis Kandiros is a fourth ye
 ar graduate student in the Theory Group at MIT EECS\, advised by Constanti
 nos Daskalakis. Before that\, he obtained his undergraduate diploma in Ele
 ctrical and Computer Engineering from the National Technical University of
  Athens. His research interests are in the fields of Statistical Learning 
 Theory and High Dimensional Probability. He is particularly interested in 
 designing and analyzing machine learning algorithms that take into account
  the dependencies between the input data. He is also interested in learnin
 g latent variable models\, specifically in providing rigorous guarantees f
 or various heuristic procedures that are commonly used in these settings\,
  such as Expectation-Maximization. He is a recipient of the Paris Kanellak
 is Fellowship and the Eric and Wendy Schmidt Center Fellowship at the Broa
 d Institute of MIT and Harvard. ​\n\nIn-person presentation\, viewable r
 emotely here:\nhttps://yale.hosted.panopto.com/Panopto/Pages/Viewer.aspx?i
 d=352c74db-ae57-422c-adfc-af3f0104beeb\n\n.\n\n\nhttps://yins.yale.edu/eve
 nt/yins-seminar-vardis-kandiros-mit
LOCATION:Yale Institute for Network Science
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-vardis-kandiros-mit
END:VEVENT
BEGIN:VEVENT
SUMMARY:FDS Seminar: Michael Lopez (NFL)
DTSTART;VALUE=DATE-TIME:20221102T200000
DTEND;VALUE=DATE-TIME:20221102T210000
DESCRIPTION:Event description:\nFDS Seminar\n\n“Analyzing the National F
 ootball League is challenging\, but player tracking data is here to help
 ”\n\nSpeaker: Michael Lopez\nSr. Director of Football Data/Analytics\nN
 ational Football League\n\nAbstract:\nMost historical National Football Le
 ague (NFL) analysis\, both mainstream and academic\, has relied on play-by
 -play data to generate team and player-level trends. Given the number of o
 utside variables that impact on-field results\, such as play call and game
  situation\, findings are often no more than interesting anecdotes. With t
 he release of player tracking data\, however\, analysts can appropriately 
 ask and answer questions that better isolate player skill and coaching str
 ategy. In this talk\, we highlight the limitations of traditional analyses
 \, and use a decades-old punching bag for analysts – fourth-down strateg
 y – as a microcosm for why tracking data is needed.\n\nSpeaker Bio:\nMic
 hael Lopez is a Senior Director of Football Data and Analytics at the Nati
 onal Football League. At the National Football League\, his work centers o
 n how to use data to enhance and better understand the game of football.\n
 \nThis is an in-person event\, with a remote access option:\n\nhttps://yal
 e.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=2adcd8f2-132c-41ca-abb3-
 af3900d8fe0d\n\n.\n\n\nhttps://yins.yale.edu/event/fds-seminar-michael-lop
 ez-nfl
LOCATION:DL220
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/fds-seminar-michael-lopez-nfl
END:VEVENT
BEGIN:VEVENT
SUMMARY:FDS Seminar: Priya Panda
DTSTART;VALUE=DATE-TIME:20221109T210000
DTEND;VALUE=DATE-TIME:20221109T220000
DESCRIPTION:Event description:\nFDS Seminar\n\n“Exploring Robustness and
  Energy-Efficiency in Neural Systems with Spike-based Machine Intelligence
 ”\n\nSpeaker: Priya Panda\nAssistant Professor\, Electrical Engineering\
 , Yale University\n\nAbstract:\nSpiking Neural Networks (SNNs) have recent
 ly emerged as an alternative to deep learning due to their huge energy eff
 iciency benefits on neuromorphic hardware. In this presentation\, I will t
 alk about important techniques for training SNNs which bring a huge benefi
 t in terms of latency\, accuracy\, interpretability\, and robustness. We w
 ill first delve into how training is performed in SNNs. Training SNNs with
  surrogate gradients presents computational benefits due to short latency.
  However\, due to the non-differentiable nature of spiking neurons\, the t
 raining becomes problematic and surrogate methods have thus been limited t
 o shallow networks. To address this training issue with surrogate gradient
 s\, we will go over a recently proposed method Batch Normalization Through
  Time (BNTT) that allows us to train SNNs from scratch with very low laten
 cy and enables us to target interesting applications like video segmentati
 on and beyond traditional learning scenarios\, like federated training. An
 other critical limitation of SNNs is the lack of interpretability. While a
  considerable amount of attention has been given to optimizing SNNs\, the 
 development of explainability still is at its infancy. I will talk about o
 ur recent work on a bio-plausible visualization tool for SNNs\, called Spi
 ke Activation Map (SAM) compatible with BNTT training. The proposed SAM hi
 ghlights spikes having short inter-spike interval\, containing discriminat
 ive information for classification. Finally\, with proposed BNTT and SAM\,
  I will highlight the robustness aspect of SNNs with respect to adversaria
 l attacks. In the end\, I will talk about interesting prospects of SNNs fo
 r non-conventional learning scenarios such as privacy-preserving distribut
 ed learning as well as unraveling the temporal correlation in SNNs with fe
 edback connections. Finally\, time permitting\, I will talk about the pros
 pects of SNNs for novel and emerging compute-in-memory hardware that can p
 otentially yield order of magnitude lower power consumption than conventio
 nal CPUs/GPUs.\n\nBio:\nPriya Panda is an assistant professor in the elect
 rical engineering department at Yale University\, USA. She received her B.
 E. and Master’s degree from BITS\, Pilani\, India in 2013 and her PhD fr
 om Purdue University\, USA in 2019. During her PhD\, she interned in Intel
  Labs where she developed large scale spiking neural network algorithms fo
 r benchmarking the Loihi chip. She is the recipient of the 2019 Amazon Res
 earch Award\, 2022 Google Research Scholar Award\, 2022 DARPA Riser Award.
  Her research interests lie in Neuromorphic Computing\, energy-efficient a
 ccelerators\, in-memory processing among others.\n\nIn-person talk\, but r
 emote access available here:\nhttps://yale.hosted.panopto.com/Panopto/Page
 s/Viewer.aspx?id=f20086eb-012d-4ead-a949-af1c01268d6d\n\n.\n\n\nhttps://yi
 ns.yale.edu/event/fds-seminar-priya-panda
LOCATION:DL220
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/fds-seminar-priya-panda
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Florian Ederer
DTSTART;VALUE=DATE-TIME:20221130T170000
DTEND;VALUE=DATE-TIME:20221130T180000
DESCRIPTION:Event description:\nYINS Seminar\n\n“A Tale of Two Networks:
  Common Ownership and Product Market Rivalry”\n\nSpeaker: Florian Ederer
 \nAssociate Professor of Economics\, Yale School of Management\n\n\n\nAbst
 ract:\nWe study the welfare implications of the rise of common ownership i
 n the United States from 1994 to 2018. We build a general equilibrium mode
 l with a hedonic demand system in which firms compete in a network game of
  oligopoly. Firms are connected through two large networks: the first refl
 ects ownership overlap\, the second product market rivalry. In our model\,
  common ownership of competing firms induces unilateral incentives to soft
 en competition. The magnitude of the common ownership effect depends on ho
 w much the two networks overlap. We estimate our model for the universe of
  U.S. public corporations using a combination of firm financials\, investo
 r holdings\, and text-based product similarity data. We perform counterfac
 tual calculations to evaluate how the efficiency and the distributional im
 pact of common ownership have evolved over time. According to our baseline
  estimates the welfare cost of common ownership\, measured as the ratio of
  deadweight loss to total surplus\, has increased nearly tenfold (from 0.3
 % to over 4%) between 1994 and 2018. Under alternative assumptions about g
 overnance\, the deadweight loss ranges between 1.9% and 4.4% of total surp
 lus in 2018. The rise of common ownership has also resulted in a significa
 nt reallocation of surplus from consumers to producers.\n\n\n\nhttps://www
 .nber.org/papers/w30004\n\n\n\n\nSpeaker Bio:\nFlorian Ederer is an Associ
 ate Professor of Economics at the Yale School of Management\, a Research S
 taff Member at the Cowles Foundation for Research in Economics\, and an NB
 ER faculty research fellow. Professor Ederer’s research\, which has been
  widely published in leading journals\, is in the areas of organizational 
 economics\, innovation\, and antitrust. Some of his recent work explores t
 he impact of networks of common ownership on managerial compensation and p
 roduct market competition and the existence and pervasiveness of “killer
  acquisitions” that prevent startups from challenging dominant market in
 cumbents. In his academic work he draws on a broad set of tools often comb
 ining theoretical models\, experimental methods\, and empirical analysis. 
 Prior to joining the Yale School of Management Professor Ederer was a facu
 lty member of the UCLA Anderson School of Management. He earned his doctor
 ate in economics at the Massachusetts Institute for Technology and his mas
 ter’s and undergraduate degrees from the University of Oxford.\n\n\n\nTh
 is is an in-person event\, with remote access available here:\nhttps://yal
 e.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1e08bc99-13d1-4da1-9e8d-
 af4100f1e59f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\nhtt
 ps://yins.yale.edu/event/yins-seminar-florian-ederer
LOCATION:Yale Institute for Network Science
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-florian-ederer
END:VEVENT
BEGIN:VEVENT
SUMMARY:FDS Seminar: John Langford (Microsoft Research)
DTSTART;VALUE=DATE-TIME:20221207T210000
DTEND;VALUE=DATE-TIME:20221207T220000
DESCRIPTION:Event description:\nFDS Seminar\n\nSpeaker: John Langford\nMic
 rosoft Research New York\n\nThis is an in-person seminar\, with remote acc
 ess available.\n\n\nhttps://yins.yale.edu/event/fds-seminar-john-langford-
 microsoft-research
LOCATION:DL220
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/fds-seminar-john-langford-microsoft-resear
 ch
END:VEVENT
BEGIN:VEVENT
SUMMARY:FDS Seminar: Robert Schapire (Microsoft)
DTSTART;VALUE=DATE-TIME:20221214T210000
DTEND;VALUE=DATE-TIME:20221214T220000
DESCRIPTION:Event description:\nFDS Seminar: Robert Schapire (Microsoft Re
 search)\n\n\nhttps://yins.yale.edu/event/fds-seminar-robert-schapire-micro
 soft
LOCATION:DL220
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/fds-seminar-robert-schapire-microsoft
END:VEVENT
END:VCALENDAR
