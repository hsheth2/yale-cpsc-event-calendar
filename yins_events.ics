BEGIN:VCALENDAR
X-WR-CALDESC:Yale Institute for Network Sciences (YINS)
X-WR-CALNAME:YINS Events
BEGIN:VEVENT
SUMMARY:YINS Alumnae Seminar: Anup Rao (Adobe Research)
DTSTART;VALUE=DATE-TIME:20210811T160000
DTEND;VALUE=DATE-TIME:20210811T170000
DESCRIPTION:Event description:\nYINS Alumnae Seminar\n\n“Machine Unlearn
 ing via Algorithmic Stability”\n\nSpeaker: Anup Rao\n\nResearch Scientis
 t\, Adobe Research\nFormerly\, beloved YINS graduate student advised by Da
 niel Spielman\n\nTalk summary:\nWe study the problem of machine unlearning
 \, updating a trained machine learning model when part of the data is dele
 ted. We identify a notion of algorithmic stability\, Total Variation (TV) 
 stability\,  and show why it is suitable for the goal of exact unlearning
 . For convex risk minimization problems\, we design TV-stable algorithms b
 ased on noisy Stochastic Gradient Descent (SGD). Our key contribution is t
 he design of corresponding efficient unlearning algorithms\, and are based
  on constructing a (maximal) coupling of Markov chains for the noisy SGD p
 rocedure. To understand the trade-offs between accuracy and unlearning eff
 iciency\, we give upper and lower bounds on excess empirical and populatio
 n risk of TV stable algorithms for convex risk minimization.   This is ba
 sed on joint work with Enayat Ullah\, Tung Mai\, Ryan Rossi and Raman Aror
 a\, and  will be presented at COLT 2021.\n\nTo participate:\n\nJoin from 
 PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.us/j/92015011527\nOr
  Telephone：203-432-9666 (2-ZOOM if on-campus) or 646 568 7788\nMeeting I
 D: 920 1501 1527\nInternational numbers available:\nhttps://yale.zoom.us/u
 /acHGdkWp2U\n\n.\n\n.\n\n\nhttps://yins.yale.edu/event/yins-alumnae-semina
 r-anup-rao-adobe-research
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-alumnae-seminar-anup-rao-adobe-resear
 ch
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Alumnae Seminar: Georgios Isofidis (TU Delft)
DTSTART;VALUE=DATE-TIME:20210818T160000
DTEND;VALUE=DATE-TIME:20210818T170000
DESCRIPTION:Event description:\nYINS Alumnae Seminar\n\nSpeaker: Georgios 
 Iosifidis\nTU Delft\n\nJoin from PC\, Mac\, Linux\, iOS or Android:\nhttps
 ://yale.zoom.us/j/96219229916\n\nOr Telephone：203-432-9666 (2-ZOOM if on
 -campus) or 646 568 7788\n\nMeeting ID: 962 1922 9916\n\nInternational num
 bers available:\nhttps://yale.zoom.us/u/adSYBvBvjM\n\n\n\n\n\n\n\n\n\n\n\n
 \n\n\n\n\n\n\n\n\n\n.\n\n\nhttps://yins.yale.edu/event/yins-alumnae-semina
 r-georgios-isofidis-tu-delft
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-alumnae-seminar-georgios-isofidis-tu-
 delft
END:VEVENT
BEGIN:VEVENT
SUMMARY:YINS Seminar: Daniel Kuhn (EPFL)
DTSTART;VALUE=DATE-TIME:20210922T160000
DTEND;VALUE=DATE-TIME:20210922T170000
DESCRIPTION:Event description:\nYINS Seminar:\n“Wasserstein Distribution
 ally Robust Optimization: Theory and Applications in Machine Learning”\n
 \nSpeaker: Daniel Kuhn (EPFL)\n\nChair of Risk Analytics and Optimization 
 at EPFL\n\nAbstract:\nMany decision problems in science\, engineering and 
 economics are affected by uncertain parameters whose distribution is only 
 indirectly observable through samples. The goal of data-driven decision-ma
 king is to learn a decision from finitely many training samples that will 
 perform well on unseen test samples. This learning task is difficult even 
 if all training and test samples are drawn from the same distribution - es
 pecially if the dimension of the uncertainty is large relative to the trai
 ning sample size. Wasserstein distributionally robust optimization seeks d
 ata-driven decisions that perform well under the most adverse distribution
  within a certain Wasserstein distance from a nominal distribution constru
 cted from the training samples. In this talk we will see that this approac
 h has many conceptual and computational benefits. Most prominently\, the o
 ptimal decisions can often be computed by solving tractable convex optimiz
 ation problems\, and they enjoy rigorous out-of-sample and asymptotic cons
 istency guarantees. We will also show that Wasserstein distributionally ro
 bust optimization has interesting ramifications for statistical learning a
 nd motivates new approaches for fundamental learning tasks such as classif
 ication\, regression\, maximum likelihood estimation or minimum mean squar
 e error estimation\, among others.\n\nSpeaker bio:\nDaniel Kuhn holds the 
 Chair of Risk Analytics and Optimization at EPFL. Before joining EPFL\, he
  was a faculty member at Imperial College London (2007-2013) and a postdoc
 toral researcher at Stanford University (2005-2006). He received a PhD in 
 Economics from the University of St. Gallen in 2004 and an MSc in Theoreti
 cal Physics from ETH Zurich in 1999. His research interests revolve around
  robust optimization and stochastic programming.\n\nTo participate:\nJoin 
 from PC\, Mac\, Linux\, iOS or Android:\nhttps://yale.zoom.us/j/9915754365
 3\nOr Telephone：203-432-9666 (2-ZOOM if on-campus) or 646 568 7788\nMeet
 ing ID: 991 5754 3653\nInternational numbers available:\nhttps://yale.zoom
 .us/u/abrK6kHkDy\n\n.\n\n\nhttps://yins.yale.edu/event/yins-seminar-daniel
 -kuhn-epfl
LOCATION:TBA
STATUS:CONFIRMED
URL:https://yins.yale.edu/event/yins-seminar-daniel-kuhn-epfl
END:VEVENT
END:VCALENDAR
