BEGIN:VCALENDAR
X-WR-CALDESC:Yale Department of Computer Science
X-WR-CALNAME:Yale CS Events
BEGIN:VEVENT
SUMMARY:Dissertation Defense - Meiying Qin
DTSTART;VALUE=DATE-TIME:20220818T140000
DTEND;VALUE=DATE-TIME:20220818T150000
DESCRIPTION:Event description:\nDissertation Defense\nMeiying Qin\n\nTitle
 : Robot Tool Use\n\nAdvisor: Brian Scassellati\n\nOther committee members:
 \nAaron Dollar\nDragomir Radev\nGoldie Nejat (University of Toronto)\n\nAb
 stract:\n\nUsing human tools can significantly benefit robots in many appl
 ication domains. It will endow a home robot with the ability to carry out 
 everyday household activities such as cleaning. It will enable an industri
 al robot to work in manufacturing or maintenance using human tools such as
  screwdrivers. It will allow a robot to perform experiments using existing
  human lab tools in a chemistry lab. Such ability would allow robots to so
 lve problems that they were unable to without tools.\n\nHowever\, robot to
 ol use is a challenging task. Tool use was initially considered to be the 
 ability that distinguishes human beings from other animals (Oakley\, 1944)
 . Robot tool use has three challenges: perception\, manipulation\, and cog
 nition. While both general manipulation tasks and tool use tasks require t
 he same level of perception accuracy\, there are unique manipulation and c
 ognition challenges in robot tool use.\n\nIn this dissertation\, we first 
 define robot tool use and compile a taxonomy of robot tool use. We identif
 y required skills for each sub-type in the taxonomy and review previous st
 udies on robot tool use based on the taxonomy. Next\, we demonstrate our w
 ork on solving some of the sub-types of robot tool use. We present an inte
 grated system that trains a robot with tool use\, transfers the learned sk
 ills to novel objects\, and can even improvise the usage of novel tools. T
 he robot that utilizes this system can also generalize the learned skills 
 to other robot platforms without additional training. We then investigate 
 more complicated forms of robot tool use. Specifically\, we explore how a 
 robot can plan sequential tool use so that the robot can use one tool to r
 etrieve another tool in order to complete a task. We also examine how a ro
 bot should choose the most appropriate tool from many possible tool option
 s. Finally\, we investigate the application of learned tool use skills in 
 related applications such as human-robot collaborations.\n\nWe conclude th
 is dissertation with the contributions and limitations of our work. We als
 o identify open challenges that remain to be solved as future directions.\
 n\n\nhttps://cpsc.yale.edu/event/dissertation-defense-meiying-qin
LOCATION:AKW 200 and Zoom
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/dissertation-defense-meiying-qin
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dissertation Defense - Wolf Honoré
DTSTART;VALUE=DATE-TIME:20220819T140000
DTEND;VALUE=DATE-TIME:20220819T150000
DESCRIPTION:Event description:\nDissertation Defense\nWolf Honoré\n\nTitl
 e: The Atomic Distributed Object Model for Distributed System Verification
 \n\nAdvisor: Zhong Shao\n\nOther committee members:\nRuzica Piskac\nJames 
 Aspnes\nBenjamin Pierce (University of Pennsylvania)\n\nAbstract:\n\nDistr
 ibuted systems are at the heart of most web-based applications and are res
 ponsible for replicating and maintaining critical data. Unfortunately\, th
 eir inherent concurrency combined with an asynchronous and unreliable netw
 ork makes them prone to implementation bugs that can have serious real-wor
 ld consequences. Formal verification can offer strong assurances of correc
 tness\; however\, despite recent advances\, reasoning directly about a sys
 tem’s implementation remains prohibitively complex. The key is to find t
 he right abstraction that faithfully models a system’s behaviors\, while
  avoiding irrelevant implementation details.\n\nThis dissertation presents
  such an abstraction called the atomic distributed object (ADO) model\, wh
 ich hides the existence of the network and reduces all behaviors to three 
 atomic operations. This not only makes the ADO model simpler\, which enabl
 es more scalable verification\, but it also means it is general enough to 
 capture a wide variety of systems. We describe three verification framewor
 ks built around the ADO model\, each implemented in the Coq proof assistan
 t and targeted at different problems. The first\, Advert\, supports compos
 itional reasoning about distributed objects\, which can be combined to bui
 ld more complex applications. The second\, Adore\, proves the safety of a 
 general class of reconfiguration schemes\, which is an essential\, but oft
 en overlooked\, operation for practical distributed systems. Finally\, Ado
 B shows that the ADO model can be used for liveness reasoning\, and can ex
 press both benign and byzantine failure models in a unified manner.\n\n\nh
 ttps://cpsc.yale.edu/event/dissertation-defense-wolf-honore
LOCATION:Zoom Presentation
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/dissertation-defense-wolf-honore
END:VEVENT
BEGIN:VEVENT
SUMMARY:Dissertation Defense - Zeyu Wang
DTSTART;VALUE=DATE-TIME:20220830T140000
DTEND;VALUE=DATE-TIME:20220830T150000
DESCRIPTION:Event description:\nDissertation Defense\nZeyu Wang\n\nTitle: 
 Enhancing the Creative Process in Digital Prototyping\n\nAdvisor: Julie Do
 rsey\n\nOther Committee Members:\nHolly Rushmeier\nSteven Zucker\nLeonard 
 McMillan (The University of North Carolina at Chapel Hill)\n\nAbstract:\n\
 nDespite advances in computer-aided design (CAD) systems and video editing
  software\, digital content creation for design\, storytelling\, and inter
 active experiences remains a challenging problem. This dissertation introd
 uces a series of studies\, techniques\, and systems along three thrusts th
 at engage creators more directly and enhance the user experience in author
 ing digital content.\n\nFirst\, we present a drawing dataset and spatiotem
 poral analysis that provide insight into how people draw by comparing trac
 ing\, freehand drawing\, and computer-generated approximations. We found a
  high degree of similarity in stroke placement and types of strokes used o
 ver time\, which informs methods for customized stroke treatment and emula
 ting drawing processes. We also propose a deep learning-based technique fo
 r line drawing synthesis from animated 3D models\, where our learned style
  space and optimization-based embedding enable the generation of line draw
 ing animations while allowing interactive user control across frames.\n\nS
 econd\, we demonstrate the importance of utilizing spatial context in the 
 creative process in augmented reality through two tablet-based interfaces.
  DistanciAR enables designers to create site-specific AR experiences for r
 emote environments using LiDAR capture and new authoring modes\, such as D
 ollhouse and Peek. PointShopAR integrates point cloud capture and editing 
 in a single AR workflow to help users quickly prototype design ideas in th
 eir spatial context. Our user studies show that LiDAR capture and the poin
 t cloud representation in these systems can make rapid AR prototyping more
  accessible and versatile.\n\nLast\, we introduce two procedural methods t
 o generate time-based media for visual communication and storytelling. Ani
 Code supports authoring and on-the-fly consumption of personalized animati
 ons in a network-free environment via a printed code. CHER-Ob generates vi
 deo flythroughs for storytelling from annotated heterogeneous 2D and 3D da
 ta for cultural heritage. Our user studies show that these methods can ben
 efit the video-oriented digital prototyping experience and facilitate the 
 dissemination of creative and cultural ideas.\n\n\nhttps://cpsc.yale.edu/e
 vent/dissertation-defense-zeyu-wang
LOCATION:Zoom Presentation
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/dissertation-defense-zeyu-wang
END:VEVENT
END:VCALENDAR
