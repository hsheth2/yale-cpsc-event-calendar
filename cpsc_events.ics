BEGIN:VCALENDAR
X-WR-CALDESC:Yale Department of Computer Science
X-WR-CALNAME:Yale CS Events
BEGIN:VEVENT
SUMMARY:Dissertation Defense - Irene Li
DTSTART;VALUE=DATE-TIME:20220216T170000
DTEND;VALUE=DATE-TIME:20220216T180000
DESCRIPTION:Event description:\nDissertation Defense\nIrene Li\n\nTitle: N
 eural Graph Transfer Learning in Natural Language Processing Tasks\n\nAdvi
 sor: Dragomir Radev\n\nOther committee members:\nMarynel Vázquez\nYang Ca
 i\nElisa Celis\n\nAbstract:\n\nNatural language is essential in our daily 
 lives as we rely on languages to communicate and exchange information. A f
 undamental goal for natural language processing (NLP) is to let the machin
 e understand natural language to help or replace human experts to mine kno
 wledge and complete tasks.\n\nMany NLP tasks deal with sequential data. Fo
 r example\, a sentence is considered as a sequence of works. However\, not
  all tasks can be formulated using sequence models. Specifically\, graph-s
 tructured data is also fundamental in NLP\, including entity linking\, ent
 ity classification\, relation extraction\, abstractive meaning representat
 ion\, and knowledge graphs. In this scenario\,  BERT-based pretrained mod
 els may not be suitable. Graph Convolutional Neural Network (GCN) is a dee
 p neural network model designed for graphs. It has shown great potential i
 n text classification\, link prediction\, question answering and so on.\n\
 nThis dissertation presents novel graph models for NLP tasks\, including t
 ext classification\, prerequisite chain learning\, and coreference resolut
 ion. We focus on different perspectives of graph convolutional network mod
 eling: for text classification\, a novel graph construction method is prop
 osed which allows interpretability for the prediction\; for prerequisite c
 hain learning\, we propose multiple aggregation functions that utilize nei
 ghbors for better information exchange\; for coreference resolution\, we s
 tudy how graph pretraining can help when labeled data is limited. Moreover
 \, an important branch is to apply pretrained language models for the ment
 ioned tasks. So\, this dissertation also focuses on the transfer learning 
 method that generalizes pretrained models to other domains\, including med
 ical\, cross-lingual\, and web data. Finally\, we propose a new task calle
 d unsupervised cross-domain prerequisite chain learning\, and study novel 
 graph-based methods to transfer knowledge over graphs.\n\n\nhttps://cpsc.y
 ale.edu/event/dissertation-defense-irene-li
LOCATION:Zoom Presentation
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/dissertation-defense-irene-li
END:VEVENT
END:VCALENDAR
