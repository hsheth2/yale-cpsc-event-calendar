BEGIN:VCALENDAR
X-WR-CALDESC:Yale Department of Computer Science
X-WR-CALNAME:Yale CS Events
BEGIN:VEVENT
SUMMARY:CS Colloquium - Danfei Xu
DTSTART;VALUE=DATE-TIME:20210329T200000
DTEND;VALUE=DATE-TIME:20210329T210000
DESCRIPTION:Event description:\nCS Colloquium\nDanfei Xu\n\nTitle: Composi
 tional Reasoning in Robot Learning\n\nHost: Steven Zucker\n\nAbstract:\n\n
 To carry out diverse tasks in everyday human environments\, future robots 
 must generalize beyond the knowledge they are equipped with. However\, des
 pite recent  advances in “end-to-end” deep learning\, today’s robot
  learning methods are still limited to specializing in one task at a time.
  At the same time\, humans perform everyday tasks with ease. But instead o
 f learning each task in silos\, we distill reusable abstractions from our 
 daily experiences and solve new tasks by composing known building blocks. 
 Such\ncompositional reasoning\ncapability is crucial for developing future
  robots that are both competent and versatile.\n\nIn this talk\, I will pr
 esent my works on building compositional reasoning capabilities into robot
  learning systems. I will start by showing that imposing strong compositio
 nal structures (e.g.\, programs\, graphs) on end-to-end robot learning app
 roaches enables systematic generalization across long-horizon object manip
 ulation tasks. Then I will present our recent efforts to relax the structu
 ral assumptions in order to bring compositional reasoning closer to real-w
 orld settings: learning from unstructured human demonstrations and learnin
 g through trial-and-error.\n\nBio:\n\nDanfei Xu is a final-year Ph.D. stud
 ent at Stanford University advised by Fei-Fei Li and Silvio Savarese. His 
 research lies in the intersection of robotics\, computer vision\, and mach
 ine learning. His research goal is to build autonomous agents that can ope
 rate in everyday human environments. He obtained a B.S. from Columbia Univ
 ersity in 2015 and has spent time at CMU Robotics Institute\, Columbia Rob
 otics Lab\, Autodesk Research\, Zoox\, and DeepMind.\n\n*Contact\nAlicia\n
 or\nNancy\nfor Zoom link and password\n\nCS Colloquium flyer\n\n\n\nhttps:
 //cpsc.yale.edu/event/cs-colloquium-danfei-xu
LOCATION:Zoom Presentation*
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-colloquium-danfei-xu
END:VEVENT
BEGIN:VEVENT
SUMMARY:CS Colloquium - Ranjay Krishna
DTSTART;VALUE=DATE-TIME:20210331T200000
DTEND;VALUE=DATE-TIME:20210331T210000
DESCRIPTION:Event description:\nCS Colloquium\nRanjay Krishna\n\nTitle: Vi
 sual Intelligence from Human Learning\n\nHost: Marynel Vázquez\n\nAbstrac
 t:\n\nAt the core of human development is the ability to adapt to new\, pr
 eviously unseen stimuli. We comprehend new situations as a composition of 
 previously seen information and ask one another for clarification when we 
 encounter new concepts. Yet\, this ability to go beyond the confounds of t
 heir training data remains an open challenge for artificial intelligence a
 gents. My research designs visual intelligence to reason over new composit
 ions and acquire new concepts. My talk will explore these challenges and p
 resent the two following lines of work:\n\nFirst\, I will introduce scene 
 graphs\, a cognitively-grounded\, compositional visual representation. I w
 ill discuss how to integrate scene graphs into a variety of computer visio
 n tasks\, enabling models to generalize to novel compositions from a few t
 raining examples. Since our introduction of scene graphs\, the Computer Vi
 sion community has developed hundreds of scene graph models and utilized s
 cene graphs to achieve state-of-the-art results across multiple core tasks
 \, including object localization\, captioning\, image generation\, questio
 n answering\, 3D understanding\, and spatio-temporal action recognition.\n
 \nSecond\, I will introduce a framework for socially situated learning. Th
 is framework pushes agents beyond traditional computer vision training par
 adigms and enables learning from human interactions in online social envir
 onments. I will showcase a real-world deployment of our agent\, which lear
 ned to acquire new visual concepts by asking people targeted questions on 
 social media. By interacting with over 230K people over 8 months\, our age
 nt learned to recognize hundreds of new concepts. This work demonstrates t
 he possibility for agents to adapt and self-improve in real-world social e
 nvironments.\n\nBio:\n\nRanjay Krishna is a 5th-year Ph.D. candidate at St
 anford University\, where he is co-advised by Fei-Fei Li and Michael Berns
 tein. His research lies at the intersection of computer vision and human-c
 omputer interaction\; it draws on ideas from behavioral and social science
 s to improve visual intelligence. His work has been recognized by the Chri
 stofer Stephenson Memorial award\, as an Accell Innovation Scholar and by 
 two Brown Institute for Media Innovation grants. His work has also been fe
 atured in Forbes magazine and in a PBS NOVA documentary. During his Ph.D.\
 , he re-designed Stanford’s undergraduate Computer Vision course and cur
 rently also instructs the graduate Computer Vision course\, Stanford’s s
 econd largest course. He received an M.Sc. from Stanford University. Befor
 e that\, he conferred a B.Sc. with a double major in Electrical Engineerin
 g and in Computer Science from Cornell University. In the past\, he has in
 terned at Google AI\, Facebook AI Research\, and Yahoo Research.\n\n*Conta
 ct\nAlicia\nor\nNancy\nfor Zoom link and password\n\nCS Colloquium flyer\n
 \n\n\n\n\nhttps://cpsc.yale.edu/event/cs-colloquium-ranjay-krishna
LOCATION:Zoom Presentation*
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-colloquium-ranjay-krishna
END:VEVENT
BEGIN:VEVENT
SUMMARY:CS Colloquium - Fraser Brown
DTSTART;VALUE=DATE-TIME:20210405T200000
DTEND;VALUE=DATE-TIME:20210405T210000
DESCRIPTION:Event description:\nCS Colloquium\nFraser Brown\n\nTitle: Elim
 inating bugs in real systems\n\nHost: Holly Rushmeier\n\nAbstract:\n\nSoft
 ware is everywhere\, and almost everywhere\, software is broken. Some bugs
  just crash your printer\; others hand an identity thief your bank account
  number\; still others let nation-states spy on dissidents and persecute m
 inorities.\n\nThis talk outlines my work preventing bugs using a blend of 
 programming languages techniques and systems design. First\, I’ll talk a
 bout securing massive\, security-critical codebases without clean slate re
 writes. This means rooting out hard-to-find bugs—as in Sys\, which scale
 s symbolic execution to find exploitable bugs in systems like the twenty-m
 illion line Chrome browser. It also means proving correctness of especiall
 y vulnerable pieces of code—as in VeRA\, which automatically verifies pa
 rt of the Firefox JavaScript engine. Finally\, I’ll discuss work on stro
 nger foundations for new systems—as in CirC\, a recent project unifying 
 compiler infrastructure for program verification\, cryptographic proofs\, 
 optimization problems\, and more.\n\nBio:\n\nFraser Brown is a PhD student
  at Stanford advised by Dawson Engler\, occasional visiting student at UCS
 D with Deian Stefan\, and NSF graduate research fellowship recipient. She 
 works at the intersection of programming languages\, systems\, and securit
 y\, and her research has been used by several companies. She holds an unde
 rgraduate degree in English from Stanford.\n\n*Contact\nAlicia\nor\nNancy\
 nfor Zoom link and password\n\nCS Colloquium flyer\n\n\n\nhttps://cpsc.yal
 e.edu/event/cs-colloquium-fraser-brown
LOCATION:Zoom Presentation*
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-colloquium-fraser-brown
END:VEVENT
BEGIN:VEVENT
SUMMARY:CS Colloquium - Kalesha Bullard
DTSTART;VALUE=DATE-TIME:20210407T200000
DTEND;VALUE=DATE-TIME:20210407T210000
DESCRIPTION:Event description:\nCS Colloquium\nKalesha Bullard\n\nTitle: 
  Learning through Interaction in Cooperative Multi-Agent Systems\n\nHost: 
 Marynel Vázquez\n\nAbstract:\n\nEffective communication is an important s
 kill for enabling information exchange and cooperation in multi-agent syst
 ems\, in which agents coexist in shared environments with humans and/or ot
 her artificial agents.  Indeed\, human domain experts can be a highly inf
 ormative source of instructive guidance and feedback (supervision).  My p
 rior work explores this type of interaction in depth\, as a mechanism for 
 enabling learning for artificial agents.  However\, dependence upon human
  partners for acquiring or adapting skills has important limitations.  Hu
 man time and cognitive load is typically constrained (particularly in real
 istic settings) and data collection from humans\, though potentially quali
 tatively rich\, can be slow and costly to acquire. Yet\, the ability to le
 arn through interaction with other agents represents another powerful mech
 anism for enabling interactive learning.  Though other artificial agents 
 may\nalso\nbe novices\, agents can co-learn through providing each other e
 valuative feedback (reinforcement)\, given the learning task has been suff
 iciently structured and allows for generalization to novel settings.\n\nTh
 is talk presents research that investigates methods for enabling agents to
  learn\ngeneral\ncommunication skills through interactions with\nother age
 nts\n.  In particular\, the talk will focus on my ongoing work within Mul
 ti-Agent Reinforcement Learning\, investigating\nemergent\ncommunication p
 rotocols\, inspired by communication in real-world problem settings.  We 
 present a novel problem setting and a general approach that allows for zer
 o-shot coordination (ZSC)\, i.e.\, discovering protocols that can generali
 ze to independently trained agents.  We also explore and analyze specific
  difficulties associated with finding globally optimal ZSC protocols\, as 
 complexity of the communication task increases or the modality for communi
 cation changes (\ne.g.\nto implicit communication through physical movemen
 t\, by a simulated robotic agent).  Overall\, this work opens up exciting
  avenues for learning\ngeneral\ncommunication protocols in complex domains
 .\n\nBio:\n\nKalesha Bullard is a postdoctoral researcher at Facebook AI R
 esearch.  She completed her PhD in Computer Science at Georgia Institute 
 of Technology in 2019\, where her research focused within the space of int
 eractive robot learning.  During her postdoc\, Kalesha has expanded her r
 esearch to explore the space of multi-agent reinforcement learning\, curre
 ntly investigating how to enable embodied multi-agent populations to learn
 \ngeneral\ncommunication protocols.  More broadly\, Kalesha’s research 
 interests span autonomous reasoning and decision making for artificial age
 nts in multi-agent settings.  To date\, her research has focused on princ
 ipled methods for enabling agents to learn through interaction with other 
 agents (human or artificial) to achieve shared goals. Beyond research\, Ka
 lesha has participated in a number of service roles throughout her researc
 h career\, recently serving on organizing and program committees for works
 hops associated with several top Artificial Intelligence conference venues
  (NeurIPS\, AAAI\, AAMAS).  This past year\, she was selected as one of t
 he 2020 Electrical Engineering and Computer Science (EECS) Rising Stars.\n
 \n*Contact\nAlicia\nor\nNancy\nfor Zoom link and password\n\nCS Colloquium
  flyer\n\n\n\n\n\nhttps://cpsc.yale.edu/event/cs-colloquium-kalesha-bullar
 d
LOCATION:Zoom Presentation*
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-colloquium-kalesha-bullard
END:VEVENT
END:VCALENDAR
