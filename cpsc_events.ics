BEGIN:VCALENDAR
X-WR-CALDESC:Yale Department of Computer Science
X-WR-CALNAME:Yale CS Events
BEGIN:VEVENT
SUMMARY:CS Talk - Dr. Tesca Fitzgerald
DTSTART;VALUE=DATE-TIME:20220314T200000
DTEND;VALUE=DATE-TIME:20220314T210000
DESCRIPTION:Event description:\nCS Talk\nDr. Tesca Fitzgerald\n\nHost: Bri
 an Scassellati\n\nTitle: Learning to address novel situations through huma
 n-robot collaboration\n\nAbstract\n\nAs our expectations for robots’ ada
 ptive capacities grow\, it will be increasingly important for\nthem to rea
 son about the novel objects\, tasks\, and interactions inherent to everyda
 y life. Rather\nthan attempt to pre-train a robot for all potential task v
 ariations it may encounter\, we can\ndevelop more capable and robust robot
 s by assuming they will inevitably encounter situations\nthat they are ini
 tially unprepared to address. My work enables a robot to address these nov
 el\nsituations by learning from a human teacher’s domain knowledge of th
 e task\, such as the\ncontextual use of an object or tool. Meeting this ch
 allenge requires robots to be flexible not only\nto novelty\, but to diffe
 rent forms of novelty and their varying effects on the robot’s task\ncom
 pletion. In this talk\, I will focus on (1) the implications of novelty\, 
 and its various causes\, on\nthe robot’s learning goals\, (2) methods fo
 r structuring its interaction with the human teacher in\norder to meet tho
 se learning goals\, and (3) modeling and learning from interaction-derived
 \ntraining data to address novelty.\n\nBio\n\nDr. Tesca Fitzgerald\nis a P
 ostdoctoral Fellow in the Robotics Institute at Carnegie Mellon\nUniversit
 y. Her research is centered around interactive robot learning\, with the a
 im of developing\nrobots that are adaptive\, robust\, and collaborative wh
 en faced with novel situations. Before\njoining Carnegie Mellon\, Dr. Fitz
 gerald received her PhD in Computer Science at Georgia Tech\nand completed
  her B.Sc at Portland State University. She is an NSF Graduate Research Fe
 llow\n(2014)\, Microsoft Graduate Women Scholar (2014)\, and IBM Ph.D. Fel
 low (2017).\n\nTalk flyer\n\n\nhttps://cpsc.yale.edu/event/cs-talk-dr-tesc
 a-fitzgerald
LOCATION:Zoom Presentation
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-talk-dr-tesca-fitzgerald
END:VEVENT
BEGIN:VEVENT
SUMMARY:CS Talk - Yue Wang
DTSTART;VALUE=DATE-TIME:20220315T200000
DTEND;VALUE=DATE-TIME:20220315T210000
DESCRIPTION:Event description:\nCS Talk\nYue Wang\n\nHost: Steven Zucker\n
 \nTitle: Learning 3D representations with minimal supervision\n\nAbstract:
 \n\nDeep learning has demonstrated considerable success embedding images a
 nd more general 2D representations into compact feature spaces for downstr
 eam tasks like recognition\, registration\, and generation. Learning on 3D
  data\, however\, is the missing piece needed for embodied agents to perce
 ive their surrounding environments. To bridge the gap between 3D perceptio
 n and robotic intelligence\, my present efforts focus on learning 3D repre
 sentations with minimal supervision.\n\nIn this talk\, I will discuss two 
 key aspects to reduce the amount of human supervision in current 3D deep l
 earning algorithms. First\, I will talk about how to recover structures fr
 om 3D data such as point clouds and incorporate such inductive bias into p
 oint cloud learning pipelines. Second\, I will present our works on levera
 ging natural supervision in point clouds to perform self-supervised learni
 ng. In addition\, I will discuss how these 3D learning algorithms enable h
 uman-level perception for robotic applications such as self-driving cars.
   Finally\, the talk will conclude with a discussion about future inquiri
 es to design complete and active 3D learning systems.\n\nBio:\n\nYue Wang 
 is a final year PhD student with Prof. Justin Solomon at MIT. His research
  interests lie in the intersection of computer vision\, computer graphics\
 , and machine learning. His major field is learning from point clouds. His
  paper “Dynamic Graph CNN” has been widely adopted in 3D visual comput
 ing and other fields. He is a recipient of the Nvidia Fellowship and is na
 med the first place recipient of the William A. Martin Master’s Thesis A
 ward for 2021. Yue received his BEng from Zhejiang University and MS from 
 University of California\, San Diego. He has spent time at Nvidia Research
 \, Google Research and Salesforce Research.\n\nTalk flyer\n\n\nhttps://cps
 c.yale.edu/event/cs-talk-yue-wang
LOCATION:Zoom Presentation
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-talk-yue-wang
END:VEVENT
BEGIN:VEVENT
SUMMARY:CS Talk - Alane Suhr
DTSTART;VALUE=DATE-TIME:20220317T143000
DTEND;VALUE=DATE-TIME:20220317T153000
DESCRIPTION:Event description:\nCS Talk\nAlane Suhr\n\nHost: Holly Rushmei
 er\n\nTitle: Reasoning and Learning in Interactive Natural Language System
 s\n\nAbstract\n\nSystems that support expressive\, situated natural langua
 ge interactions are essential for expanding access to complex computing sy
 stems\, such as robots and databases\, to non-experts. Reasoning and learn
 ing in such natural language interactions is a challenging open problem. F
 or example\, resolving sentence meaning requires reasoning not only about 
 word meaning\, but also about the interaction context\, including the hist
 ory of the interaction and the situated environment. In addition\, the seq
 uential dynamics that arise between user and system in and across interact
 ions make learning from static data\, i.e.\, supervised data\, both challe
 nging and ineffective. However\, these same interaction dynamics result in
  ample opportunities for learning from implicit and explicit feedback that
  arises naturally in the interaction. This lays the foundation for systems
  that continually learn\, improve\, and adapt their language use through i
 nteraction\, without additional annotation effort. In this talk\, I will f
 ocus on these challenges and opportunities. First\, I will describe our wo
 rk on modeling dependencies between language meaning and interaction conte
 xt when mapping natural language in interaction to executable code. In the
  second part of the talk\, I will describe our work on language understand
 ing and generation in collaborative environments\, focusing on continual l
 earning from explicit and implicit user feedback.\n\nBio\n\nAlane Suhr is 
 a PhD Candidate in the Department of Computer Science at Cornell Universit
 y\,  advised by Yoav Artzi. Her research spans natural language processin
 g\, machine learning\, and computer vision\, with a focus on building syst
 ems that participate and continually learn in situated natural language in
 teractions with human users. Alane’s work has been recognized by paper a
 wards at ACL and NAACL\, and has been supported by fellowships and grants\
 , including an NSF Graduate Research Fellowship\, a Facebook PhD Fellowshi
 p\, and research awards from AI2\, ParlAI\, and AWS. Alane has also co-org
 anized multiple workshops and tutorials appearing at NeurIPS\, EMNLP\, NAA
 CL\, and ACL. Previously\, Alane received a BS in Computer Science and Eng
 ineering as an Eminence Fellow at the Ohio State University.\n\nTalk flyer
 \n\n\nhttps://cpsc.yale.edu/event/cs-talk-alane-suhr
LOCATION:TBA
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-talk-alane-suhr
END:VEVENT
BEGIN:VEVENT
SUMMARY:CS Talk - Alex Wong
DTSTART;VALUE=DATE-TIME:20220328T200000
DTEND;VALUE=DATE-TIME:20220328T210000
DESCRIPTION:Event description:\nCS Talk\nAlex Wong\n\nHost: Steve Zucker\n
 \nTitle: Towards Depth Perception for Embodied Cognition\n\nAbstract:\n\nE
 mbodied cognition entails the ability to adapt one’s understanding of th
 e physical world through interactions with the surrounding space. Amongst 
 the many aspects of intelligence encompassed by embodied cognition\, we fo
 cus on depth perception to support agents performing spatial tasks. While 
 deep learning has seen a number of empirical successes\, many of the exist
 ing works are not suitable for realizing embodied cognition due to (i) the
 ir computational requirements i.e. model sizes up to trillions of paramete
 rs\, (ii) their need for expensive human annotations as supervision\, and 
 (iii) their sensitivity to small perturbations in their inputs. To address
  these areas\, we begin by proposing a method that enables an agent to lea
 rn to infer the structure of the 3-dimensional scene from multi-sensory ob
 servations – online\, causally\, and without human supervision. By lever
 aging priors about our physical world during the learning process or as an
  inductive bias\, we show that it is not only possible to reduce the model
  size\, but also gain performance to yield real-time depth perception syst
 ems with state-of-the-art accuracies. These priors can also improve the ro
 bustness of such systems against common perturbations of their inputs as w
 ell as adversarial perturbations that are designed to disrupt their normal
  operations. The culmination of our work is being realized in an interdepa
 rtmental collaboration at UCLA to build the first fully autonomous catarac
 t surgery robot. We plan to take another step towards embodied cognition b
 y building on top of my progress in depth perception to enable an agent to
  learn the semantics of objects populating the scene without human interve
 ntion.\n\nBio:\n\nAlex Wong is a postdoctoral scholar at the University of
  California\, Los Angeles (UCLA) under the guidance of Stefano Soatto. He 
 received his Ph.D. from UCLA\, and was co-advised by Stefano Soatto and Al
 an Yuille. His research lies in the intersection of machine learning\, com
 puter vision\, and robotics. His work has received the outstanding student
  paper award at the Conference on Neural Information Processing Systems (N
 eurIPS) 2011 and the best paper award in robot vision at the International
  Conference on Robotics and Automation (ICRA) 2019.\n\nTalk flyer\n\n\nhtt
 ps://cpsc.yale.edu/event/cs-talk-alex-wong
LOCATION:Zoom Presentation
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-talk-alex-wong
END:VEVENT
BEGIN:VEVENT
SUMMARY:CS Talk - Hao Peng
DTSTART;VALUE=DATE-TIME:20220329T200000
DTEND;VALUE=DATE-TIME:20220329T210000
DESCRIPTION:Event description:\nCS Talk\nHao Peng\n\nHost: Dragomir Radev\
 n\nTitle: Towards Efficient and Generalizable Natural Language Processing\
 n\nAbstract:\n\nLarge-scale deep learning models have become the foundatio
 n of today’s natural language processing (NLP). Despite their recent\, t
 remendous success\, they struggle with generalization in real-world settin
 gs\, like their predecessors. Besides\, their sheer scale brings new chall
 enges—the increasing computational cost heightens the barriers to entry 
 to NLP research.\n\nThe first part of the talk will discuss innovations in
  neural architectures that can help address the efficiency concerns of tod
 ay’s NLP. I will present algorithms that reduce state-of-the-art NLP mod
 els’ overhead from quadratic to linear in input lengths without hurting 
 accuracy. Second\, I will turn to inductive biases grounded in the inheren
 t structure of natural language sentences\, which can help machine learnin
 g models generalize. I will discuss the integration of discrete\, symbolic
  structure prediction into modern deep learning.\n\nI will conclude with f
 uture directions towards making cutting-edge NLP more efficient\, and impr
 oving NLP’s generalization to serve today’s language technology applic
 ations and those to come in the future.\n\nBio:\n\nHao Peng is a final yea
 r PhD student in Computer Science & Engineering at the University of Washi
 ngton\, advised by Noah A. Smith. His research focuses on building efficie
 nt\, generalizable\, and interpretable machine learning models for natural
  language processing. His research has been presented at top-tier natural 
 language processing and machine learning venues\, and recognized with a Go
 ogle PhD fellowship and an honorable mention for the best paper at ACL 201
 8.\n\nTalk flyer\n\n\nhttps://cpsc.yale.edu/event/cs-talk-hao-peng
LOCATION:TBA
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-talk-hao-peng
END:VEVENT
BEGIN:VEVENT
SUMMARY:CS Colloquium - Richard L. Sites
DTSTART;VALUE=DATE-TIME:20220405T200000
DTEND;VALUE=DATE-TIME:20220405T210000
DESCRIPTION:Event description:\nCS Colloquium\nRichard L. Sites\n\nHost: M
 ichael Fischer\n\nTitle: Making the Invisible Visible\nObserving Complex S
 oftware Dynamics\n\nAbstract\n\nFrom mobile and cloud apps to video games 
 to driverless vehicle control\, more and more software is time-constrained
 : it must deliver reliable results seamlessly\, consistently\, and virtual
 ly instantaneously. If it doesn’t\, customers are unhappy–and sometime
 s lives are put at risk. When complex software underperforms or fails\, id
 entifying the root causes is difficult and\, historically\, few tools have
  been available to help\, leaving application developers to guess what mig
 ht be happening. How can we do better?\n\nThe key is to have low-overhead 
 observation tools that can show exactly where all the  elapsed time goes 
 in both normal responses and in delayed responses. Doing so makes visible 
 each of the seven possible reasons for such delays\, as we show.\n\nBio\n\
 nRichard L. Sites wrote his first computer program in 1959 and has spent m
 ost of his career at the boundary between hardware and software\, with a p
 articular interest in CPU/software performance interactions. His past work
  includes VAX microcode\, DEC Alpha co-architect\, and inventing the perfo
 rmance counters found in nearly all processors today. He has done low-over
 head microcode and software tracing at DEC\, Adobe\, Google\, and Tesla. D
 r. Sites earned his PhD at Stanford in 1974\; he holds 66 patents and is a
  member of the US National Academy of Engineering.\n\n\nhttps://cpsc.yale.
 edu/event/cs-colloquium-richard-l-sites
LOCATION:AKW 200
STATUS:CONFIRMED
URL:https://cpsc.yale.edu/event/cs-colloquium-richard-l-sites
END:VEVENT
END:VCALENDAR
